# Stats
- 24 files
- 5243 (5.2K) lines
- 166547 (167K) chars
- 19493 (19K) `whitespace-split` tokens

# File Tree

```
python-project-makefile-template    
├── .github                         
│   └── workflows                   
│    ├── checks.yml                 [113L  2,154C   216T]
│    └── makefile-checks.yml        [104L  2,763C   339T]
├── myproject                       
│   ├── __init__.py                 [  1L     32C     3T]
│   ├── helloworld.py               [ 16L    429C    59T]
│   └── other.py                    [  7L    168C    23T]
├── notebooks                       
│   ├── example.ipynb               [ 66L  1,154C   133T]
│   └── no_desc.ipynb               [ 34L    640C    72T]
├── scripts                         
│   ├── make                        
│   │   ├── check_torch.py          [186L  5,732C   560T]
│   │   ├── docs_clean.py           [103L  3,229C   359T]
│   │   ├── export_requirements.py  [141L  3,895C   445T]
│   │   ├── generate_badge.py       [320L  7,624C   840T]
│   │   ├── get_commit_log.py       [ 48L  1,177C   120T]
│   │   ├── get_todos.py            [436L 11,977C 1,368T]
│   │   ├── get_version.py          [ 31L    948C    91T]
│   │   ├── make_docs.py            [505L 15,434C 1,619T]
│   │   ├── pdoc_markdown2_cli.py   [ 72L  2,081C   200T]
│   │   ├── recipe_info.py          [537L 15,376C 1,745T]
│   │   └── typing_breakdown.py     [386L 12,446C 1,241T]
│   └── assemble.py                 [ 76L  2,292C   231T]
├── tests                           
│   └── test_nothing.py             [  2L     42C     5T]
├── README.md                       [150L  6,880C   912T]
├── makefile                        [706L 27,863C 3,636T]
├── makefile.template               [706L 27,809C 3,633T]
├── pyproject.toml                  [367L 10,415C 1,313T]
```

# File Contents

``````{ path=".github/workflows/checks.yml"  }
name: Checks

on:
  workflow_dispatch:
  pull_request:
    branches:
      - '*'
  push:
    branches:
      - main

jobs:
  dep-check:
    name: Check dependencies
    runs-on: ubuntu-latest
    strategy:
      matrix:
        versions:
          - python: "3.11"
          # - python: "3.12"
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0 # whole history for making version

      - uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.versions.python }}

      - name: Set up uv
        uses: astral-sh/setup-uv@v5
        with:
          enable-cache: true

      - name: print python version
        run: python --version

      - name: check deps
        run: make dep-check

      - name: make info-long
        run: make info-long

  lint:
    name: Lint
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Set up uv
        uses: astral-sh/setup-uv@v5
        with:
          enable-cache: true

      - name: install
        run: make setup

      - name: format-check
        run: make format-check

  test:
    name: Test
    runs-on: ubuntu-latest
    strategy:
      matrix:
        versions:
          - python: "3.11"
          # - python: "3.12"
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.versions.python }}

      - name: Set up uv
        uses: astral-sh/setup-uv@v5
        with:
          enable-cache: true

      - name: install
        run: make setup

      - name: Unit tests
        run: make test

  build:
    name: Typing & Build
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Set up uv
        uses: astral-sh/setup-uv@v5
        with:
          enable-cache: true

      - name: install
        run: make setup

      - name: Type checking
        run: make typing

      - name: Build package
        run: make build

``````{ end_of_file=".github/workflows/checks.yml" }

``````{ path=".github/workflows/makefile-checks.yml"  }
name: Checks

on:
  workflow_dispatch:
  pull_request:
    branches:
      - '*'
  push:
    branches:
      - main

jobs:
  test-make:
    name: Test Make Recipes
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false  # Continue with other versions if one fails
      matrix:
        versions:
          - python: "3.9"
          - python: "3.10"
          - python: "3.11"
          - python: "3.12"
          - python: "3.13"	  
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for version commands
      
      - uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.versions.python }}

      - name: Setup
        run: |
          curl -LsSf https://astral.sh/uv/install.sh | sh
          make setup

      - name: Install system dependencies
        uses: awalsh128/cache-apt-pkgs-action@latest
        with:
          packages: pandoc
          version: '3.3'

      # Info commands
      - run: make
      - run: make help
      - run: make info
      - run: make info-long
      - run: make version

      # Test detailed help system
      - name: Test detailed help for single target
        run: make help=test
      - name: Test detailed help for multiple targets
        run: make HELP="test clean"
      - name: Test detailed help with pattern matching
        run: make help="dep*"
      - name: Test detailed help with wildcard
        run: make h="*clean"
      - name: Test detailed help for all targets
        run: make H=--all
      - name: Test recipe_info.py script directly
        run: python scripts/make/recipe_info.py -f makefile test
      - name: Test recipe_info.py with --no-color
        run: python scripts/make/recipe_info.py -f makefile --no-color test
      - name: Test recipe_info.py error handling
        run: python scripts/make/recipe_info.py -f makefile nonexistent 2>&1 | grep -q "not found"

      # Dependency commands  
      - run: make dep-check
      - run: make dep
      - run: make dep-check-torch

      # Code quality commands
      - run: make format-check
      - run: make format 
      - run: make typing
      - run: make typing-summary

      # Build commands
      - run: make build
      - run: make verify-git

      # Testing commands
      - run: make test
      - run: make check
      - run: make cov

      # Documentation commands
      - run: make docs-html
      - run: make docs-md
      - run: make docs-combined
      - run: make docs
      - run: make todo
      # these two will not work under python < 3.11
      - run: make lmcat-tree
      - run: make lmcat

      # Cleanup commands  
      - run: make docs-clean
      - run: make clean
      - run: make dep-clean
      - run: make clean-all
``````{ end_of_file=".github/workflows/makefile-checks.yml" }

``````{ path="myproject/__init__.py"  }
""".. include:: ../README.md"""

``````{ end_of_file="myproject/__init__.py" }

``````{ path="myproject/helloworld.py"  }
"dummy module"

print("hello world")


# another line which should be included in the body
# TODO: an example todo that `make todo` should find
def some_function() -> None:
	"dummy docstring"
	raise NotImplementedError("This function is not implemented yet")


# FIXME: an example that `make todo` should find
def critical_function() -> None:
	"dummy docstring"
	raise NotImplementedError("This function is not implemented yet")

``````{ end_of_file="myproject/helloworld.py" }

``````{ path="myproject/other.py"  }
"a module"


# BUG: make todo should see this too
def another_function() -> None:
	"dummy docstring"
	raise NotImplementedError("This function is not implemented yet")

``````{ end_of_file="myproject/other.py" }

``````{ path="notebooks/example.ipynb"  }
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# this notebook just shows you can export them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you will be able to see the output of the cells\n"
     ]
    }
   ],
   "source": [
    "print(\"you will be able to see the output of the cells\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

``````{ end_of_file="notebooks/example.ipynb" }

``````{ path="notebooks/no_desc.ipynb"  }
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this notebook will have no description on the index, but you can still see it"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

``````{ end_of_file="notebooks/no_desc.ipynb" }

``````{ path="scripts/make/check_torch.py"  }
# python project makefile template
# https://github.com/mivanit/python-project-makefile-template
# version: ##[[VERSION]]##
# license: https://creativecommons.org/licenses/by-sa/4.0/

"""Print info about current python, torch, cuda, and devices.

Useful for debugging environment issues and verifying GPU availability.
"""

from __future__ import annotations

import os
import re
import subprocess
import sys
from typing import TYPE_CHECKING, Any, Callable

if TYPE_CHECKING:
	from collections.abc import Mapping


def print_info_dict(
	info: Mapping[str, str | int | Mapping[str, Any]],
	indent: str = "  ",
	level: int = 1,
) -> None:
	"pretty print the info"
	indent_str: str = indent * level
	longest_key_len: int = max(map(len, info.keys()))
	for key, value in info.items():
		if isinstance(value, dict):
			print(f"{indent_str}{key:<{longest_key_len}}:")
			print_info_dict(value, indent, level + 1)
		else:
			print(f"{indent_str}{key:<{longest_key_len}} = {value}")


def get_nvcc_info() -> dict[str, str]:
	"get info about cuda from nvcc --version"
	# Run the nvcc command.
	try:
		result: subprocess.CompletedProcess[str] = subprocess.run(
			["nvcc", "--version"],  # noqa: S607
			check=True,
			capture_output=True,
			text=True,
		)
	except Exception as e:
		return {"Failed to run 'nvcc --version'": str(e)}

	output: str = result.stdout
	lines: list[str] = [line.strip() for line in output.splitlines() if line.strip()]

	# Ensure there are exactly 5 lines in the output.
	if len(lines) != 5:
		msg = f"Expected exactly 5 lines from nvcc --version, got {len(lines)} lines:\n{output}"
		raise ValueError(msg)

	# Compile shared regex for release info.
	release_regex: re.Pattern[str] = re.compile(
		r"Cuda compilation tools,\s*release\s*([^,]+),\s*(V.+)",
	)

	# Define a mapping for each desired field:
	# key -> (line index, regex pattern, group index, transformation function)
	patterns: dict[str, tuple[int, re.Pattern[str], int, Callable[[str], str]]] = {
		"build_time": (
			2,
			re.compile(r"Built on (.+)"),
			1,
			lambda s: s.replace("_", " "),
		),
		"release": (3, release_regex, 1, str.strip),
		"release_V": (3, release_regex, 2, str.strip),
		"build": (4, re.compile(r"Build (.+)"), 1, str.strip),
	}

	info: dict[str, str] = {}
	for key, (line_index, pattern, group_index, transform) in patterns.items():
		match: re.Match[str] | None = pattern.search(lines[line_index])
		if not match:
			err_msg: str = (
				f"Unable to parse {key} from nvcc output: {lines[line_index]}"
			)
			raise ValueError(err_msg)
		info[key] = transform(match.group(group_index))

	info["release_short"] = info["release"].replace(".", "").strip()

	return info


def get_torch_info() -> tuple[list[Exception], dict[str, Any]]:
	"get info about pytorch and cuda devices"
	exceptions: list[Exception] = []
	info: dict[str, Any] = {}

	try:
		import torch  # type: ignore[import-not-found] # noqa: PLC0415
	except ImportError as e:
		info["torch.__version__"] = "not available"
		exceptions.append(e)
		return exceptions, info

	try:
		info["torch.__version__"] = torch.__version__
		info["torch.cuda.is_available()"] = torch.cuda.is_available()

		if torch.cuda.is_available():
			info["torch.version.cuda"] = torch.version.cuda
			info["torch.cuda.device_count()"] = torch.cuda.device_count()

			if torch.cuda.device_count() > 0:
				info["torch.cuda.current_device()"] = torch.cuda.current_device()
				n_devices: int = torch.cuda.device_count()
				info["n_devices"] = n_devices
				for current_device in range(n_devices):
					try:
						current_device_info: dict[str, str | int] = {}

						dev_prop = torch.cuda.get_device_properties(  # pyright: ignore[reportUnknownVariableType,reportUnknownMemberType]
							torch.device(f"cuda:{current_device}"),
						)

						current_device_info["name"] = dev_prop.name  # pyright: ignore[reportUnknownMemberType]
						current_device_info["version"] = (
							f"{dev_prop.major}.{dev_prop.minor}"  # pyright: ignore[reportUnknownMemberType]
						)
						current_device_info["total_memory"] = (
							f"{dev_prop.total_memory} ({dev_prop.total_memory:.1e})"  # pyright: ignore[reportUnknownMemberType]
						)
						current_device_info["multi_processor_count"] = (
							dev_prop.multi_processor_count  # pyright: ignore[reportUnknownMemberType]
						)
						current_device_info["is_integrated"] = dev_prop.is_integrated  # pyright: ignore[reportUnknownMemberType]
						current_device_info["is_multi_gpu_board"] = (
							dev_prop.is_multi_gpu_board  # pyright: ignore[reportUnknownMemberType]
						)

						info[f"device cuda:{current_device}"] = current_device_info

					except Exception as e:  # noqa: PERF203
						exceptions.append(e)
			else:
				err_msg_nodevice: str = (
					f"{torch.cuda.device_count() = } devices detected, invalid"
				)
				raise ValueError(err_msg_nodevice)  # noqa: TRY301

		else:
			err_msg_nocuda: str = (
				f"CUDA is NOT available in torch: {torch.cuda.is_available() = }"
			)
			raise ValueError(err_msg_nocuda)  # noqa: TRY301

	except Exception as e:
		exceptions.append(e)

	return exceptions, info


if __name__ == "__main__":
	print(f"python: {sys.version}")
	print_info_dict(
		{
			"python executable path: sys.executable": str(sys.executable),
			"sys.platform": sys.platform,
			"current working directory: os.getcwd()": os.getcwd(),
			"Host name: os.name": os.name,
			"CPU count: os.cpu_count()": str(os.cpu_count()),
		},
	)

	nvcc_info: dict[str, Any] = get_nvcc_info()
	print("nvcc:")
	print_info_dict(nvcc_info)

	torch_exceptions, torch_info = get_torch_info()
	print("torch:")
	print_info_dict(torch_info)

	if torch_exceptions:
		print("torch_exceptions:")
		for e in torch_exceptions:
			print(f"  {e}")

``````{ end_of_file="scripts/make/check_torch.py" }

``````{ path="scripts/make/docs_clean.py"  }
# python project makefile template
# https://github.com/mivanit/python-project-makefile-template
# version: ##[[VERSION]]##
# license: https://creativecommons.org/licenses/by-sa/4.0/

"""Clean up docs directory based on pyproject.toml configuration.

Removes generated documentation files while preserving resources and
files specified in [tool.makefile.docs.no_clean].

Usage: python docs_clean.py <pyproject_path> <docs_dir> [extra_preserve...]
"""

from __future__ import annotations

import shutil
import sys
from functools import reduce
from pathlib import Path
from typing import Any, cast

try:
	import tomllib  # type: ignore[import-not-found]
except ImportError:
	import tomli as tomllib  # type: ignore[import-untyped,import-not-found,no-redef] # pyright: ignore[reportMissingImports]

TOOL_PATH: str = "tool.makefile.docs"
DEFAULT_DOCS_DIR: str = "docs"


def deep_get(
	d: dict[str, Any],
	path: str,
	default: Any = None,  # noqa: ANN401
	sep: str = ".",
) -> Any:  # noqa: ANN401
	"""Get nested dictionary value via separated path with default."""
	return reduce(
		lambda x, y: x.get(y, default) if isinstance(x, dict) else default,  # function
		path.split(sep) if isinstance(path, str) else path,  # sequence
		d,  # initial
	)


def read_config(pyproject_path: Path) -> tuple[Path, set[Path]]:
	"read configuration from pyproject.toml"
	if not pyproject_path.is_file():
		return Path(DEFAULT_DOCS_DIR), set()

	with pyproject_path.open("rb") as f:
		config: dict[str, Any] = cast("dict[str, Any]", tomllib.load(f))  # pyright: ignore[reportUnknownMemberType]

	preserved: list[str] = deep_get(config, f"{TOOL_PATH}.no_clean", [])
	docs_dir: Path = Path(deep_get(config, f"{TOOL_PATH}.output_dir", DEFAULT_DOCS_DIR))

	# Convert to absolute paths and validate
	preserve_set: set[Path] = set()
	for p in preserved:
		full_path = (docs_dir / p).resolve()
		if not full_path.as_posix().startswith(docs_dir.resolve().as_posix()):
			err_msg: str = f"Preserved path '{p}' must be within docs directory"
			raise ValueError(err_msg)
		preserve_set.add(docs_dir / p)

	return docs_dir, preserve_set


def clean_docs(docs_dir: Path, preserved: set[Path]) -> None:
	"""delete files not in preserved set

	TODO: this is not recursive
	"""
	for path in docs_dir.iterdir():
		if path.is_file() and path not in preserved:
			path.unlink()
		elif path.is_dir() and path not in preserved:
			shutil.rmtree(path)


def main(
	pyproject_path: str,
	docs_dir_cli: str,
	extra_preserve: list[str],
) -> None:
	"Clean up docs directory based on pyproject.toml configuration."
	docs_dir: Path
	preserved: set[Path]
	docs_dir, preserved = read_config(Path(pyproject_path))

	if not docs_dir.is_dir():
		msg = f"Docs directory '{docs_dir}' not found"
		raise FileNotFoundError(msg)
	if docs_dir != Path(docs_dir_cli):
		msg = f"Docs directory mismatch: {docs_dir = } != {docs_dir_cli = }. this is probably because you changed one of `pyproject.toml:{TOOL_PATH}.output_dir` (the former) or `makefile:DOCS_DIR` (the latter) without updating the other."
		raise ValueError(msg)

	for x in extra_preserve:
		preserved.add(Path(x))
	clean_docs(docs_dir, preserved)


if __name__ == "__main__":
	main(sys.argv[1], sys.argv[2], sys.argv[3:])

``````{ end_of_file="scripts/make/docs_clean.py" }

``````{ path="scripts/make/export_requirements.py"  }
# python project makefile template
# https://github.com/mivanit/python-project-makefile-template
# version: ##[[VERSION]]##
# license: https://creativecommons.org/licenses/by-sa/4.0/

"""Export dependencies to requirements.txt files based on pyproject.toml configuration.

Reads [tool.makefile.uv-exports] from pyproject.toml and generates uv export
commands for each configured export. Output is shell commands printed to stdout.

Usage: python export_requirements.py <pyproject_path> <output_dir>
"""

from __future__ import annotations

import sys
import warnings

try:
	import tomllib  # type: ignore[import-not-found]
except ImportError:
	import tomli as tomllib  # type: ignore[import-not-found,no-redef,import-untyped]  # pyright: ignore[reportMissingImports]
from functools import reduce
from pathlib import Path
from typing import Any, cast

TOOL_PATH: str = "tool.makefile.uv-exports"


def deep_get(
	d: dict[str, Any],
	path: str,
	default: Any = None,  # noqa: ANN401
	sep: str = ".",
) -> Any:  # noqa: ANN401
	"get a value from a nested dictionary"
	return reduce(
		lambda x, y: x.get(y, default) if isinstance(x, dict) else default,  # function
		path.split(sep) if isinstance(path, str) else path,  # sequence
		d,  # initial
	)


def export_configuration(
	export: dict[str, Any],
	all_groups: list[str],
	all_extras: list[str],
	export_opts: dict[str, Any],
	output_dir: Path,
) -> None:
	"print to console a uv command for make which will export a requirements.txt file"
	# get name and validate
	name = export.get("name")
	if not name or not name.isalnum():
		warnings.warn(
			f"Export configuration missing valid 'name' field {export}",
		)
		return

	# get other options with default fallbacks
	filename: str = export.get("filename") or f"requirements-{name}.txt"
	groups: list[str] | bool | None = export.get("groups")
	extras: list[str] | bool = export.get("extras", [])
	options: list[str] = export.get("options", [])

	# init command
	cmd: list[str] = ["uv", "export", *export_opts.get("args", [])]

	# handle groups
	if groups is not None:
		groups_list: list[str] = []
		if isinstance(groups, bool):
			if groups:
				groups_list = all_groups.copy()
		else:
			groups_list = groups

		for group in all_groups:
			if group in groups_list:
				cmd.extend(["--group", group])
			else:
				cmd.extend(["--no-group", group])

	# handle extras
	extras_list: list[str] = []
	if isinstance(extras, bool):
		if extras:
			extras_list = all_extras.copy()
	else:
		extras_list = extras

	for extra in extras_list:
		cmd.extend(["--extra", extra])

	# add extra options
	cmd.extend(options)

	# assemble the command and print to console -- makefile will run it
	output_path = output_dir / filename
	print(f"{' '.join(cmd)} > {output_path.as_posix()}")


def main(
	pyproject_path: Path,
	output_dir: Path,
) -> None:
	"export to requirements.txt files based on pyproject.toml configuration"
	# read pyproject.toml
	with open(pyproject_path, "rb") as f:
		pyproject_data: dict[str, Any] = cast("dict[str, Any]", tomllib.load(f))  # pyright: ignore[reportUnknownMemberType]

	# all available groups
	all_groups: list[str] = list(pyproject_data.get("dependency-groups", {}).keys())
	all_extras: list[str] = list(
		deep_get(pyproject_data, "project.optional-dependencies", {}).keys(),
	)

	# options for exporting
	export_opts: dict[str, Any] = deep_get(pyproject_data, TOOL_PATH, {})

	# what are we exporting?
	exports: list[dict[str, Any]] = export_opts.get("exports", [])
	if not exports:
		exports = [{"name": "all", "groups": [], "extras": [], "options": []}]

	# export each configuration
	for export in exports:
		export_configuration(
			export=export,
			all_groups=all_groups,
			all_extras=all_extras,
			export_opts=export_opts,
			output_dir=output_dir,
		)


if __name__ == "__main__":
	main(
		pyproject_path=Path(sys.argv[1]),
		output_dir=Path(sys.argv[2]),
	)

``````{ end_of_file="scripts/make/export_requirements.py" }

``````{ path="scripts/make/generate_badge.py"  }
# python project makefile template
# https://github.com/mivanit/python-project-makefile-template
# version: ##[[VERSION]]##
# license: https://creativecommons.org/licenses/by-sa/4.0/

"""Generate GitHub-style SVG badges for coverage, tests, or arbitrary label/value pairs.

Usage:
    python generate_badge.py [OPTIONS]

Examples:
    # Generic badge
    python generate_badge.py --label "version" --value "1.2.3"
    python generate_badge.py --label "license" --value "MIT" --color blue

    # Coverage badge (reads from coverage.txt report)
    python generate_badge.py --coverage coverage.txt

    # Tests badge (parses pytest output)
    python generate_badge.py --pytest-results .pytest_results.txt

"""

from __future__ import annotations

import argparse
import re
import sys
from pathlib import Path

# Color presets (GitHub badge style)
COLORS: dict[str, str] = {
	"brightgreen": "#4c1",
	"green": "#97ca00",
	"yellowgreen": "#a4a61d",
	"yellow": "#dfb317",
	"orange": "#fe7d37",
	"red": "#e05d44",
	"blue": "#007ec6",
	"gray": "#555",
	"lightgray": "#9f9f9f",
}


def get_coverage_color(percent: float) -> str:
	"""Get color based on coverage percentage."""
	if percent >= 80:
		return COLORS["brightgreen"]
	elif percent >= 60:
		return COLORS["yellowgreen"]
	elif percent >= 40:
		return COLORS["orange"]
	else:
		return COLORS["red"]


def get_tests_color(passed: int, failed: int) -> str:
	"""Get color based on test results."""
	if failed > 0:
		return COLORS["red"]
	elif passed > 0:
		return COLORS["brightgreen"]
	else:
		return COLORS["gray"]


def estimate_text_width(text: str) -> int:
	"""Estimate text width in pixels for the badge font.

	Uses approximate character widths for DejaVu Sans 11px.
	"""
	# Approximate widths for common characters
	narrow_chars = "iIl1|!.,;:'"
	wide_chars = "mwMWOQGD%@"

	width = 0
	for char in text:
		if char in narrow_chars:
			width += 4
		elif char in wide_chars:
			width += 10
		elif char.isupper():
			width += 8
		else:
			width += 7

	return width + 10  # Add padding


def generate_badge_svg(label: str, value: str, color: str) -> str:
	"""Generate a GitHub-style flat badge SVG.

	Args:
		label: Left side text (e.g., "coverage", "tests")
		value: Right side text (e.g., "85%", "42 passed")
		color: Hex color for the value background (with or without #)

	Returns:
		SVG string

	"""
	if not color.startswith("#"):
		color = f"#{color}"

	label_width = estimate_text_width(label)
	value_width = estimate_text_width(value)
	total_width = label_width + value_width

	label_x = label_width / 2
	value_x = label_width + value_width / 2

	svg = f"""<?xml version="1.0" encoding="UTF-8"?>
<svg xmlns="http://www.w3.org/2000/svg" width="{total_width}" height="20">
    <clipPath id="r"><rect width="{total_width}" height="20" rx="3"/></clipPath>
    <g clip-path="url(#r)">
        <rect width="{label_width}" height="20" fill="#555"/>
        <rect x="{label_width}" width="{value_width}" height="20" fill="{color}"/>
    </g>
    <g fill="#fff" text-anchor="middle" font-family="DejaVu Sans,Verdana,Geneva,sans-serif" font-size="11">
        <text x="{label_x}" y="14">{label}</text>
        <text x="{value_x}" y="14">{value}</text>
    </g>
</svg>
"""
	return svg


def read_coverage_from_txt(coverage_path: Path) -> float:
	"""Read coverage percentage from coverage.txt report.

	Args:
		coverage_path: Path to coverage.txt file

	Returns:
		Coverage percentage (0-100)

	"""
	content = coverage_path.read_text()

	# Look for TOTAL line, e.g., "TOTAL    1234    567    54%"
	match = re.search(r"^TOTAL\s+\d+\s+\d+\s+(\d+)%", content, re.MULTILINE)
	if match:
		return float(match.group(1))

	# Try alternate format with cover column
	match = re.search(r"^TOTAL\s+\d+\s+\d+\s+\d+\s+(\d+)%", content, re.MULTILINE)
	if match:
		return float(match.group(1))

	msg = f"Could not parse coverage percentage from {coverage_path}"
	raise RuntimeError(msg)


def parse_pytest_results(results_path: Path) -> tuple[int, int, int]:
	"""Parse pytest output to get test counts.

	Args:
		results_path: Path to file containing pytest output

	Returns:
		Tuple of (passed, failed, total)

	"""
	content = results_path.read_text()

	passed = 0
	failed = 0
	skipped = 0
	errors = 0

	# Look for summary line like "42 passed, 3 failed, 1 skipped in 1.23s"
	# or "===== 42 passed in 1.23s ====="
	summary_match = re.search(
		r"=+\s*([\d\w\s,]+(?:passed|failed|error|skipped)[^=]*)\s*=+",
		content,
		re.IGNORECASE,
	)

	if summary_match:
		summary = summary_match.group(1)

		passed_match = re.search(r"(\d+)\s*passed", summary, re.IGNORECASE)
		if passed_match:
			passed = int(passed_match.group(1))

		failed_match = re.search(r"(\d+)\s*failed", summary, re.IGNORECASE)
		if failed_match:
			failed = int(failed_match.group(1))

		skipped_match = re.search(r"(\d+)\s*skipped", summary, re.IGNORECASE)
		if skipped_match:
			skipped = int(skipped_match.group(1))

		error_match = re.search(r"(\d+)\s*error", summary, re.IGNORECASE)
		if error_match:
			errors = int(error_match.group(1))

	total = passed + failed + skipped + errors
	return passed, failed + errors, total


def resolve_color(color_input: str) -> str:
	"""Resolve color name or hex code to hex value.

	Args:
		color_input: Color name (e.g., "green") or hex code (e.g., "#4c1" or "4c1")

	Returns:
		Hex color code with #

	"""
	if color_input in COLORS:
		return COLORS[color_input]
	if color_input.startswith("#"):
		return color_input
	return f"#{color_input}"


def main() -> int:
	parser = argparse.ArgumentParser(
		description="Generate GitHub-style SVG badges",
		formatter_class=argparse.RawDescriptionHelpFormatter,
		epilog=__doc__,
	)

	# Generic mode
	_ = parser.add_argument(
		"--label",
		type=str,
		help="Badge label (left side text)",
	)
	_ = parser.add_argument(
		"--value",
		type=str,
		help="Badge value (right side text)",
	)
	_ = parser.add_argument(
		"--color",
		type=str,
		default="gray",
		help="Badge color (name or hex, default: gray)",
	)

	# Coverage mode
	_ = parser.add_argument(
		"--coverage",
		type=Path,
		metavar="PATH",
		help="Path to coverage.txt report",
	)

	# Tests mode
	_ = parser.add_argument(
		"--pytest-results",
		type=Path,
		metavar="PATH",
		help="Path to file containing pytest output",
	)

	args = parser.parse_args()

	# Determine mode and generate badge
	try:
		if args.coverage:
			# Coverage mode
			coverage_path = args.coverage
			if not coverage_path.exists():
				print(
					f"Error: Coverage file not found: {coverage_path}", file=sys.stderr
				)
				return 1

			percent = read_coverage_from_txt(coverage_path)
			label = "coverage"
			value = f"{percent:.0f}%"
			color = get_coverage_color(percent)

		elif args.pytest_results:
			# Tests mode
			if not args.pytest_results.exists():
				print(
					f"Error: Pytest results file not found: {args.pytest_results}",
					file=sys.stderr,
				)
				return 1

			passed, failed, total = parse_pytest_results(args.pytest_results)
			label = "tests"

			if failed > 0:
				value = f"{passed}/{total} passed"
			else:
				value = f"{passed} passed"

			color = get_tests_color(passed, failed)

		elif args.label and args.value:
			# Generic mode
			label = args.label
			value = args.value
			color = resolve_color(args.color)

		else:
			parser.error(
				"Must specify either --coverage, --pytest-results, or both --label and --value"
			)
			return 1  # pyright: ignore[reportUnreachable]

		svg = generate_badge_svg(label, value, color)
		print(svg)

		return 0

	except Exception as e:
		print(f"Error: {e}", file=sys.stderr)
		return 1


if __name__ == "__main__":
	sys.exit(main())

``````{ end_of_file="scripts/make/generate_badge.py" }

``````{ path="scripts/make/get_commit_log.py"  }
# python project makefile template
# https://github.com/mivanit/python-project-makefile-template
# version: ##[[VERSION]]##
# license: https://creativecommons.org/licenses/by-sa/4.0/

"""Generate a formatted commit log and write it to a file.

Usage: python get_commit_log.py <last_version> <output_file>
"""

from __future__ import annotations

import subprocess
import sys


def main(
	last_version: str,
	commit_log_file: str,
) -> None:
	"pretty print a commit log amd wrote it to a file"
	if last_version == "NULL":
		print("!!! ERROR !!!", file=sys.stderr)
		print("LAST_VERSION is NULL, can't get commit log!", file=sys.stderr)
		sys.exit(1)

	try:
		log_cmd: list[str] = [
			"git",
			"log",
			f"{last_version}..HEAD",
			"--pretty=format:- %s (%h)",
		]
		commits: list[str] = (
			subprocess.check_output(log_cmd).decode("utf-8").strip().split("\n")  # noqa: S603
		)
		with open(commit_log_file, "w") as f:
			_ = f.write("\n".join(reversed(commits)))
	except subprocess.CalledProcessError as e:
		print(f"Error: {e}", file=sys.stderr)
		sys.exit(1)


if __name__ == "__main__":
	main(
		last_version=sys.argv[1].strip(),
		commit_log_file=sys.argv[2].strip(),
	)

``````{ end_of_file="scripts/make/get_commit_log.py" }

``````{ path="scripts/make/get_todos.py"  }
# python project makefile template
# https://github.com/mivanit/python-project-makefile-template
# version: ##[[VERSION]]##
# license: https://creativecommons.org/licenses/by-sa/4.0/

"""Scrape TODO/FIXME/etc comments from source files and generate reports.

Reads configuration from [tool.makefile.inline-todo] in pyproject.toml.
Outputs markdown, jsonl, and html reports with links to source and GitHub issues.
"""

from __future__ import annotations

import argparse
import fnmatch
import json
import textwrap
import urllib.parse
import warnings
from dataclasses import asdict, dataclass, field
from functools import reduce
from pathlib import Path
from typing import Any, cast

from jinja2 import Template

try:
	import tomllib  # type: ignore[import-not-found]
except ImportError:
	import tomli as tomllib  # type: ignore[import-untyped,import-not-found,no-redef] # pyright: ignore[reportMissingImports]

TOOL_PATH: str = "tool.makefile.inline-todo"


def deep_get(
	d: dict[str, Any],
	path: str,
	default: Any = None,  # noqa: ANN401
	sep: str = ".",
) -> Any:  # noqa: ANN401
	"get a value from a nested dictionary"
	return reduce(
		lambda x, y: x.get(y, default) if isinstance(x, dict) else default,  # function
		path.split(sep) if isinstance(path, str) else path,  # sequence
		d,  # initial
	)


_TEMPLATE_MD_LIST: str = """\
# Inline TODOs

{% for tag, file_map in grouped|dictsort %}
# {{ tag }}
{% for filepath, item_list in file_map|dictsort %}
## [`{{ filepath }}`](/{{ filepath }})
{% for itm in item_list %}
- {{ itm.stripped_title }}  
  local link: [`/{{ filepath }}:{{ itm.line_num }}`](/{{ filepath }}#L{{ itm.line_num }}) 
  | view on GitHub: [{{ itm.file }}#L{{ itm.line_num }}]({{ itm.code_url | safe }})
  | [Make Issue]({{ itm.issue_url | safe }})
{% if itm.context %}
  ```{{ itm.file_lang }}
{{ itm.context_indented }}
  ```
{% endif %}
{% endfor %}

{% endfor %}
{% endfor %}
"""

_TEMPLATE_MD_TABLE: str = """\
# Inline TODOs

| Location | Tag | Todo | GitHub | Issue |
|:---------|:----|:-----|:-------|:------|
{% for itm in all_items %}| [`{{ itm.file }}:{{ itm.line_num }}`](/{{ itm.file }}#L{{ itm.line_num }}) | {{ itm.tag }} | {{ itm.stripped_title_escaped }} | [View]({{ itm.code_url | safe }}) | [Create]({{ itm.issue_url | safe }}) |
{% endfor %}
"""

TEMPLATES_MD: dict[str, str] = dict(
	standard=_TEMPLATE_MD_LIST,
	table=_TEMPLATE_MD_TABLE,
)

TEMPLATE_ISSUE: str = """\
# source

[`{file}#L{line_num}`]({code_url})

# context
```{file_lang}
{context}
```
"""


@dataclass
class Config:
	"""Configuration for the inline-todo scraper"""

	search_dir: Path = Path()
	out_file_base: Path = Path("docs/todo-inline")
	tags: list[str] = field(
		default_factory=lambda: ["CRIT", "TODO", "FIXME", "HACK", "BUG"],
	)
	extensions: list[str] = field(default_factory=lambda: ["py", "md"])
	exclude: list[str] = field(default_factory=lambda: ["docs/**", ".venv/**"])
	context_lines: int = 2
	valid_post_tag: str | list[str] = " \t:<>|[](){{}}"
	valid_pre_tag: str | list[str] = " \t:<>|[](){{}}#"
	tag_label_map: dict[str, str] = field(
		default_factory=lambda: {
			"CRIT": "bug",
			"TODO": "enhancement",
			"FIXME": "bug",
			"BUG": "bug",
			"HACK": "enhancement",
		},
	)
	extension_lang_map: dict[str, str] = field(
		default_factory=lambda: {
			"py": "python",
			"md": "markdown",
			"html": "html",
			"css": "css",
			"js": "javascript",
		},
	)

	templates_md: dict[str, str] = field(default_factory=lambda: TEMPLATES_MD)
	# templates for the output markdown file

	template_issue: str = TEMPLATE_ISSUE
	# template for the issue creation

	template_html_source: Path = Path("docs/resources/templates/todo-template.html")
	# template source for the output html file (interactive table)

	@property
	def template_html(self) -> str:
		"read the html template"
		return self.template_html_source.read_text(encoding="utf-8")

	template_code_url_: str = "{repo_url}/blob/{branch}/{file}#L{line_num}"
	# template for the code url

	@property
	def template_code_url(self) -> str:
		"code url with repo url and branch substituted"
		return self.template_code_url_.replace("{repo_url}", self.repo_url).replace(
			"{branch}",
			self.branch,
		)

	repo_url: str = "UNKNOWN"
	# for the issue creation url

	branch: str = "main"
	# branch for links to files on github

	@classmethod
	def read(cls, config_file: Path) -> Config:
		"read from a file, or return default"
		output: Config
		if config_file.is_file():
			# read file and load if present
			with config_file.open("rb") as f:
				data: dict[str, Any] = cast("dict[str, Any]", tomllib.load(f))  # pyright: ignore[reportUnknownMemberType]

			# try to get the repo url
			repo_url: str = "UNKNOWN"
			try:
				urls: dict[str, str] = {
					k.lower(): v for k, v in data["project"]["urls"].items()
				}
				if "repository" in urls:
					repo_url = urls["repository"]
				if "github" in urls:
					repo_url = urls["github"]
			except Exception as e:
				warnings.warn(
					f"No repository URL found in pyproject.toml, 'make issue' links will not work.\n{e}",
				)

			# load the inline-todo config if present
			data_inline_todo: dict[str, Any] = deep_get(
				d=data,
				path=TOOL_PATH,
				default={},
			)

			if "repo_url" not in data_inline_todo:
				data_inline_todo["repo_url"] = repo_url

			output = cls.load(data_inline_todo)
		else:
			# return default otherwise
			output = cls()

		return output

	@classmethod
	def load(cls, data: dict[str, Any]) -> Config:
		"load from a dictionary, converting to `Path` as needed"
		# process variables that should be paths
		data = {
			k: Path(v)
			if k in {"search_dir", "out_file_base", "template_html_source"}
			else v
			for k, v in data.items()
		}

		# default value for the templates
		data["templates_md"] = {
			**TEMPLATES_MD,
			**data.get("templates_md", {}),
		}

		return cls(**data)


CFG: Config = Config()
# this is messy, but we use a global config so we can get `TodoItem().issue_url` to work


@dataclass
class TodoItem:
	"""Holds one todo occurrence"""

	tag: str
	file: str
	line_num: int
	content: str
	context: str = ""

	def serialize(self) -> dict[str, str | int]:
		"serialize to a dict we can dump to json"
		return {
			**asdict(self),
			"issue_url": self.issue_url,
			"file_lang": self.file_lang,
			"stripped_title": self.stripped_title,
			"code_url": self.code_url,
		}

	@property
	def context_indented(self) -> str:
		"""Returns the context with each line indented"""
		dedented: str = textwrap.dedent(self.context)
		return textwrap.indent(dedented, "  ")

	@property
	def code_url(self) -> str:
		"""Returns a URL to the code on GitHub"""
		return CFG.template_code_url.format(
			file=self.file,
			line_num=self.line_num,
		)

	@property
	def stripped_title(self) -> str:
		"""Returns the title of the issue, stripped of the tag"""
		return self.content.split(self.tag, 1)[-1].lstrip(":").strip()

	@property
	def stripped_title_escaped(self) -> str:
		"""Returns the title of the issue, stripped of the tag and escaped for markdown"""
		return self.stripped_title.replace("|", "\\|")

	@property
	def issue_url(self) -> str:
		"""Constructs a GitHub issue creation URL for a given TodoItem."""
		# title
		title: str = self.stripped_title
		if not title:
			title = "Issue from inline todo"
		# body
		body: str = CFG.template_issue.format(
			file=self.file,
			line_num=self.line_num,
			context=self.context,
			context_indented=self.context_indented,
			code_url=self.code_url,
			file_lang=self.file_lang,
		).strip()
		# labels
		label: str = CFG.tag_label_map.get(self.tag, self.tag)
		# assemble url
		query: dict[str, str] = dict(title=title, body=body, labels=label)
		query_string: str = urllib.parse.urlencode(query, quote_via=urllib.parse.quote)
		return f"{CFG.repo_url}/issues/new?{query_string}"

	@property
	def file_lang(self) -> str:
		"""Returns the language for the file extension"""
		ext: str = Path(self.file).suffix.lstrip(".")
		return CFG.extension_lang_map.get(ext, ext)


def scrape_file(
	file_path: Path,
	cfg: Config,
) -> list[TodoItem]:
	"""Scrapes a file for lines containing any of the specified tags"""
	items: list[TodoItem] = []
	if not file_path.is_file():
		return items
	lines: list[str] = file_path.read_text(encoding="utf-8").splitlines(True)

	# over all lines
	for i, line in enumerate(lines):
		# over all tags
		for tag in cfg.tags:
			# check tag is present
			if tag in line[:200]:
				# check tag is surrounded by valid strings
				tag_idx_start: int = line.index(tag)
				tag_idx_end: int = tag_idx_start + len(tag)
				if (
					line[tag_idx_start - 1] in cfg.valid_pre_tag
					and line[tag_idx_end] in cfg.valid_post_tag
				):
					# get the context and add the item
					start: int = max(0, i - cfg.context_lines)
					end: int = min(len(lines), i + cfg.context_lines + 1)
					snippet: str = "".join(lines[start:end])
					items.append(
						TodoItem(
							tag=tag,
							file=file_path.as_posix(),
							line_num=i + 1,
							content=line.strip("\n"),
							context=snippet.strip("\n"),
						),
					)
				break
	return items


def collect_files(
	search_dir: Path,
	extensions: list[str],
	exclude: list[str],
) -> list[Path]:
	"""Recursively collects all files with specified extensions, excluding matches via globs"""
	results: list[Path] = []
	for ext in extensions:
		results.extend(search_dir.rglob(f"*.{ext}"))

	return [
		f
		for f in results
		if not any(fnmatch.fnmatch(f.as_posix(), pattern) for pattern in exclude)
	]


def group_items_by_tag_and_file(
	items: list[TodoItem],
) -> dict[str, dict[str, list[TodoItem]]]:
	"""Groups items by tag, then by file"""
	grouped: dict[str, dict[str, list[TodoItem]]] = {}
	for itm in items:
		grouped.setdefault(itm.tag, {}).setdefault(itm.file, []).append(itm)
	for tag_dict in grouped.values():
		for file_list in tag_dict.values():
			file_list.sort(key=lambda x: x.line_num)
	return grouped


def main(config_file: Path) -> None:
	"cli interface to get todos"
	global CFG  # noqa: PLW0603
	# read configuration
	cfg: Config = Config.read(config_file)
	CFG = cfg  # pyright: ignore[reportConstantRedefinition]

	# get data
	files: list[Path] = collect_files(cfg.search_dir, cfg.extensions, cfg.exclude)
	all_items: list[TodoItem] = []
	n_files: int = len(files)
	for i, fpath in enumerate(files):
		print(f"Scraping {i + 1:>2}/{n_files:>2}: {fpath.as_posix():<60}", end="\r")
		all_items.extend(scrape_file(fpath, cfg))

	# create dir
	cfg.out_file_base.parent.mkdir(parents=True, exist_ok=True)

	# write raw to jsonl
	with open(cfg.out_file_base.with_suffix(".jsonl"), "w", encoding="utf-8") as f:
		f.writelines(json.dumps(itm.serialize()) + "\n" for itm in all_items)

	# group, render
	grouped: dict[str, dict[str, list[TodoItem]]] = group_items_by_tag_and_file(
		all_items,
	)

	# render each template and save
	for template_key, template in cfg.templates_md.items():
		rendered: str = Template(template).render(grouped=grouped, all_items=all_items)
		template_out_path: Path = Path(
			cfg.out_file_base.with_stem(
				cfg.out_file_base.stem + f"-{template_key}",
			).with_suffix(".md"),
		)
		_ = template_out_path.write_text(rendered, encoding="utf-8")

	# write html output
	try:
		html_rendered: str = cfg.template_html.replace(
			"//{{DATA}}//",
			json.dumps([itm.serialize() for itm in all_items]),
		)
		_ = cfg.out_file_base.with_suffix(".html").write_text(
			html_rendered,
			encoding="utf-8",
		)
	except Exception as e:
		warnings.warn(f"Failed to write html output: {e}")

	print("wrote to:")
	print(cfg.out_file_base.with_suffix(".md").as_posix())


if __name__ == "__main__":
	# parse args
	parser: argparse.ArgumentParser = argparse.ArgumentParser("inline_todo")
	_ = parser.add_argument(
		"--config-file",
		default="pyproject.toml",
		help="Path to the TOML config, will look under [tool.inline-todo].",
	)
	args: argparse.Namespace = parser.parse_args()
	config_file: str = args.config_file
	# call main
	main(Path(config_file))

``````{ end_of_file="scripts/make/get_todos.py" }

``````{ path="scripts/make/get_version.py"  }
# python project makefile template
# https://github.com/mivanit/python-project-makefile-template
# version: ##[[VERSION]]##
# license: https://creativecommons.org/licenses/by-sa/4.0/

"""Extract version from pyproject.toml and print to stdout.

Usage: python get_version.py <pyproject_path>
Prints 'v<version>' on success, 'NULL' on failure.
"""

from __future__ import annotations

import sys
from typing import Any, cast

try:
	try:
		import tomllib  # type: ignore[import-not-found]
	except ImportError:
		import tomli as tomllib  # type: ignore[import-untyped,import-not-found,no-redef] # pyright: ignore[reportMissingImports]

	pyproject_path: str = sys.argv[1].strip()

	with open(pyproject_path, "rb") as f:
		pyproject_data: dict[str, Any] = cast("dict[str, Any]", tomllib.load(f))  # pyright: ignore[reportUnknownMemberType]

	print("v" + pyproject_data["project"]["version"], end="")
except Exception:
	print("NULL", end="")
	sys.exit(1)

``````{ end_of_file="scripts/make/get_version.py" }

``````{ path="scripts/make/make_docs.py"  }
# python project makefile template
# https://github.com/mivanit/python-project-makefile-template
# version: ##[[VERSION]]##
# license: https://creativecommons.org/licenses/by-sa/4.0/

"""Generate HTML and markdown documentation using pdoc.

Reads configuration from [tool.makefile.docs] in pyproject.toml.
Supports combined single-file markdown output and notebook conversion.

Usage: python make_docs.py [--serve] [--warn-all] [--combined]
"""

from __future__ import annotations

import argparse
import inspect  # noqa: TC003
import json
import re
import warnings
from dataclasses import asdict, dataclass, field
from functools import reduce
from pathlib import Path
from typing import Any, cast

try:
	# python 3.11+
	import tomllib  # type: ignore[import-not-found]
except ImportError:
	import tomli as tomllib  # type: ignore  # pyright: ignore[reportMissingImports]

import jinja2
import pdoc  # type: ignore[import-not-found]
import pdoc.doc  # type: ignore[import-not-found]
import pdoc.extract  # type: ignore[import-not-found]
import pdoc.render  # type: ignore[import-not-found]
import pdoc.render_helpers  # type: ignore[import-not-found]
from markupsafe import Markup

"""
 ######  ######## ######## ##     ## ########
##    ## ##          ##    ##     ## ##     ##
##       ##          ##    ##     ## ##     ##
 ######  ######      ##    ##     ## ########
      ## ##          ##    ##     ## ##
##    ## ##          ##    ##     ## ##
 ######  ########    ##     #######  ##
"""
# setup
# ============================================================

CONFIG_PATH: Path = Path("pyproject.toml")
TOOL_PATH: str = "tool.makefile.docs"

HTML_TO_MD_MAP: dict[str, str] = {
	"&gt;": ">",
	"&lt;": "<",
	"&amp;": "&",
	"&quot;": '"',
	"&#39": "'",
	"&apos;": "'",
}

pdoc.render_helpers.markdown_extensions["alerts"] = True  # type: ignore[assignment]  # pyright: ignore[reportArgumentType]
pdoc.render_helpers.markdown_extensions["admonitions"] = True  # type: ignore[assignment]  # pyright: ignore[reportArgumentType]


_CONFIG_NOTEBOOKS_INDEX_TEMPLATE: str = r"""<!doctype html>
<html lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<title>Notebooks</title>
	<link rel="stylesheet" href="../resources/css/bootstrap-reboot.min.css">
	<link rel="stylesheet" href="../resources/css/theme.css">
	<link rel="stylesheet" href="../resources/css/content.css">
</head>
<body>
	<h1>Notebooks</h1>
	<p>
		You can find the source code for the notebooks at
		<a href="{{ notebook_url }}">{{ notebook_url }}</a>.
	<ul>
		{% for notebook in notebooks %}
		<li><a href="{{ notebook.html }}">{{ notebook.ipynb }}</a> {{ notebook.desc }}</li>
		{% endfor %}
	</ul>
	<a href="../">Back to index</a>
</body>
</html>
"""


def deep_get(
	d: dict[str, Any],
	path: str,
	default: Any = None,  # noqa: ANN401
	sep: str = ".",
	warn_msg_on_default: str | None = None,
) -> Any:  # noqa: ANN401
	"Get a value from a nested dictionary"
	output: Any = reduce(
		lambda x, y: x.get(y, default) if isinstance(x, dict) else default,  # function
		path.split(sep) if isinstance(path, str) else path,  # sequence
		d,  # initial
	)

	if warn_msg_on_default and output == default:
		warnings.warn(warn_msg_on_default.format(path=path))

	return output


# CONFIGURATION -- read from CONFIG_PATH, assumed to be a pyproject.toml
# ============================================================


@dataclass
class Config:
	"""Configuration for the documentation generation

	read from a mix of package info and more specific configuration options under
	`TOOL_PATH` in the `pyproject.toml`. see `_CFG_PATHS` for the mappings
	"""

	# under main pyproject.toml
	package_name: str = "unknown"
	package_repo_url: str = "unknown"
	package_version: str = "unknown"
	# under tool_path
	output_dir_str: str = "docs"
	markdown_headings_increment: int = 2
	warnings_ignore: list[str] = field(default_factory=list)
	notebooks_enabled: bool = False
	notebooks_descriptions: dict[str, str] = field(default_factory=dict)
	notebooks_source_path_str: str = "notebooks"
	notebooks_output_path_relative_str: str = "notebooks"
	notebooks_index_template: str = _CONFIG_NOTEBOOKS_INDEX_TEMPLATE

	@property
	def package_code_url(self) -> str:
		"link to the code on the repo"
		if "unknown" not in (self.package_name, self.package_version):
			return self.package_repo_url + "/blob/" + self.package_version
		else:
			return "unknown"

	@property
	def module_name(self) -> str:
		"""name of the module, which is the package name with '-' replaced by '_'

		HACK: this is kind of fragile
		"""
		return self.package_name.replace("-", "_")

	@property
	def output_dir(self) -> Path:
		"path to write the docs to, notebooks output dir is specified relative to this"
		return Path(self.output_dir_str)

	@property
	def notebooks_source_path(self) -> Path:
		"path to read notebooks from"
		return Path(self.notebooks_source_path_str)

	@property
	def notebooks_output_path(self) -> Path:
		"path to write converted html notebooks to"
		return self.output_dir / self.notebooks_output_path_relative_str


CONFIG: Config

_CFG_PATHS: dict[str, str] = dict(
	package_name="project.name",
	package_repo_url="project.urls.Repository",
	package_version="project.version",
	output_dir_str=f"{TOOL_PATH}.output_dir",
	markdown_headings_increment=f"{TOOL_PATH}.markdown_headings_increment",
	warnings_ignore=f"{TOOL_PATH}.warnings_ignore",
	notebooks_enabled=f"{TOOL_PATH}.notebooks.enabled",
	notebooks_descriptions=f"{TOOL_PATH}.notebooks.descriptions",
	notebooks_index_template=f"{TOOL_PATH}.notebooks.index_template",
	notebooks_source_path_str=f"{TOOL_PATH}.notebooks.source_path",
	notebooks_output_path_relative_str=f"{TOOL_PATH}.notebooks.output_path_relative",
)


def set_global_config() -> Config:
	"""set global var `CONFIG` from pyproject.toml"""
	global CONFIG  # noqa: PLW0603

	# get the default and read the data
	cfg_default: Config = Config()

	with CONFIG_PATH.open("rb") as f:
		pyproject_data: dict[str, Any] = cast("dict[str, Any]", tomllib.load(f))  # pyright: ignore[reportUnknownMemberType]

	# apply the mapping from toml path to attribute
	cfg_partial: dict[str, Any] = {
		key: deep_get(
			d=pyproject_data,
			path=path,
			default=getattr(cfg_default, key),
			warn_msg_on_default=f"could not find {path}"
			if key.startswith("package")
			else None,
		)
		for key, path in _CFG_PATHS.items()
	}

	# set the global var
	CONFIG = Config(**cfg_partial)  # pyright: ignore[reportConstantRedefinition]

	# add the package meta to the pdoc globals
	pdoc.render.env.globals["package_version"] = CONFIG.package_version  # pyright: ignore[reportArgumentType]
	pdoc.render.env.globals["package_name"] = CONFIG.package_name  # pyright: ignore[reportArgumentType]
	pdoc.render.env.globals["package_repo_url"] = CONFIG.package_repo_url  # pyright: ignore[reportArgumentType]
	pdoc.render.env.globals["package_code_url"] = CONFIG.package_code_url  # pyright: ignore[reportArgumentType]

	return CONFIG


"""
##     ## ########
###   ### ##     ##
#### #### ##     ##
## ### ## ##     ##
##     ## ##     ##
##     ## ##     ##
##     ## ########
"""
# markdown
# ============================================================


def replace_heading(match: re.Match[str]) -> str:
	"replace a matched heading with an incremented version"
	current_level: int = len(match.group(1))
	new_level: int = min(
		current_level + CONFIG.markdown_headings_increment,
		6,
	)  # Cap at h6
	return "#" * new_level + match.group(2)


def increment_markdown_headings(markdown_text: str) -> str:
	"""Increment all Markdown headings in the given text by the specified amount

	# Parameters:
	- `markdown_text : str`
		The input Markdown text

	# Returns:
	- `str`
		The Markdown text with incremented heading levels.
	"""
	# Regular expression to match Markdown headings
	heading_pattern: re.Pattern[str] = re.compile(r"^(#{1,6})(.+)$", re.MULTILINE)

	# Replace all headings with incremented versions
	return heading_pattern.sub(replace_heading, markdown_text)


def format_signature(sig: inspect.Signature, colon: bool) -> Markup:
	"""Format a function signature for Markdown. Returns a single-line Markdown string."""
	# First get a list with all params as strings.
	result = pdoc.doc._PrettySignature._params(sig)  # type: ignore  # pyright: ignore[reportArgumentType,reportPrivateUsage]
	return_annot = pdoc.doc._PrettySignature._return_annotation_str(sig)  # type: ignore  # pyright: ignore[reportArgumentType,reportPrivateUsage]

	def _format_param(param: str) -> str:
		"""Format a parameter for Markdown, including potential links."""
		# This is a simplified version. You might need to adjust this
		# to properly handle links in your specific use case.
		return f"`{param}`"

	# Format each parameter
	pretty_result = [_format_param(param) for param in result]

	# Join parameters
	params_str = ", ".join(pretty_result)

	# Add return annotation
	anno = ")"
	if return_annot:
		anno += f" -> `{return_annot}`"
	if colon:
		anno += ":"

	# Construct the full signature
	return Markup(f"`(`{params_str}`{anno}`")  # noqa: S704


def markup_safe(sig: inspect.Signature) -> str:
	"mark some text as safe, no escaping needed"
	output: str = str(sig)
	# the user is marking it as safe, not our problem
	return Markup(output)  # noqa: S704


def use_markdown_format() -> None:
	"set some functions to output markdown format"
	pdoc.render_helpers.format_signature = format_signature  # type: ignore[invalid-assignment]
	pdoc.render.env.filters["markup_safe"] = markup_safe
	pdoc.render.env.filters["increment_markdown_headings"] = increment_markdown_headings


"""
##    ## ########
###   ## ##     ##
####  ## ##     ##
## ## ## ########
##  #### ##     ##
##   ### ##     ##
##    ## ########
"""


# notebook
# ============================================================
def convert_notebooks() -> None:
	"""Convert Jupyter notebooks to HTML files"""
	try:
		import nbconvert  # noqa: PLC0415
		import nbformat  # noqa: PLC0415
	except ImportError as e:
		err_msg: str = 'nbformat and nbconvert are required to convert notebooks to HTML, add "nbconvert>=7.16.4" to dev/docs deps'
		raise ImportError(err_msg) from e

	# create output directory
	CONFIG.notebooks_output_path.mkdir(parents=True, exist_ok=True)

	# read in the notebook metadata
	notebook_names: list[Path] = list(CONFIG.notebooks_source_path.glob("*.ipynb"))
	notebooks: list[dict[str, str]] = [
		dict(
			ipynb=notebook.name,
			html=notebook.with_suffix(".html").name,
			desc=CONFIG.notebooks_descriptions.get(notebook.stem, ""),
		)
		for notebook in notebook_names
	]

	# Render the index template
	template: jinja2.Template = jinja2.Template(CONFIG.notebooks_index_template)
	rendered_index: str = template.render(notebooks=notebooks)

	# Write the rendered index to a file
	index_path: Path = CONFIG.notebooks_output_path / "index.html"
	index_path.write_text(rendered_index)

	# convert with nbconvert
	for notebook in notebook_names:
		output_notebook: Path = (
			CONFIG.notebooks_output_path / notebook.with_suffix(".html").name
		)
		with open(notebook, "r") as f_in:
			nb: nbformat.NotebookNode = cast(
				"nbformat.NotebookNode",
				nbformat.read(f_in, as_version=4),  # pyright: ignore[reportUnknownMemberType]
			)
			html_exporter: nbconvert.HTMLExporter = nbconvert.HTMLExporter()  # ty: ignore[possibly-missing-attribute]
			body: str
			body, _ = html_exporter.from_notebook_node(nb)
			with open(output_notebook, "w") as f_out:
				f_out.write(body)


"""
##     ##    ###    #### ##    ##
###   ###   ## ##    ##  ###   ##
#### ####  ##   ##   ##  ####  ##
## ### ## ##     ##  ##  ## ## ##
##     ## #########  ##  ##  ####
##     ## ##     ##  ##  ##   ###
##     ## ##     ## #### ##    ##
"""
# main
# ============================================================


def pdoc_combined(*modules: str, output_file: Path) -> None:
	"""Render the documentation for a list of modules into a single HTML file.

	Args:
		*modules: Paths or names of the modules to document.
		output_file: Path to the output HTML file.

	This function will:
	1. Extract all modules and submodules.
	2. Generate documentation for each module.
	3. Combine all module documentation into a single HTML file.
	4. Write the combined documentation to the specified output file.

	Rendering options can be configured by calling `pdoc.render.configure` in advance.

	"""
	# Extract all modules and submodules
	all_modules: dict[str, pdoc.doc.Module] = {}
	for module_name in pdoc.extract.walk_specs(modules):
		all_modules[module_name] = pdoc.doc.Module.from_name(module_name)

	# Generate HTML content for each module
	module_contents: list[str] = []
	for module in all_modules.values():
		module_html = pdoc.render.html_module(module, all_modules)
		module_contents.append(module_html)

	# Combine all module contents
	combined_content = "\n".join(module_contents)

	# Write the combined content to the output file
	with output_file.open("w", encoding="utf-8") as f:
		f.write(combined_content)


def ignore_warnings() -> None:
	"Process and apply the warning filters"
	for message in CONFIG.warnings_ignore:
		warnings.filterwarnings("ignore", message=message)


if __name__ == "__main__":
	# parse args
	# --------------------------------------------------
	argparser: argparse.ArgumentParser = argparse.ArgumentParser()
	argparser.add_argument(
		"--serve",
		"-s",
		action="store_true",
		help="Whether to start an HTTP server to serve the documentation",
	)
	argparser.add_argument(
		"--warn-all",
		"-w",
		action="store_true",
		help=f"Whether to show all warnings, instead of ignoring the ones specified in pyproject.toml:{TOOL_PATH}.warnings_ignore",
	)
	argparser.add_argument(
		"--combined",
		"-c",
		action="store_true",
		help="Whether to combine the documentation for multiple modules into a single markdown file",
	)
	parsed_args = argparser.parse_args()

	# configure pdoc
	# --------------------------------------------------
	# read what we need from the pyproject.toml, add stuff to pdoc globals
	CONFIG = set_global_config()  # pyright: ignore[reportConstantRedefinition]

	# ignore warnings if needed
	if not parsed_args.warn_all:
		ignore_warnings()

	pdoc.render.configure(
		edit_url_map={
			CONFIG.package_name: CONFIG.package_code_url,
		},
		template_directory=(
			CONFIG.output_dir / "resources/templates/html/"
			if not parsed_args.combined
			else CONFIG.output_dir / "resources/templates/markdown/"
		),
		show_source=True,
		math=True,
		mermaid=True,
		search=True,
	)

	print(json.dumps(asdict(CONFIG), indent=2))

	# do the rendering
	# --------------------------------------------------
	if not parsed_args.combined:
		pdoc.pdoc(
			CONFIG.module_name,
			output_directory=CONFIG.output_dir,
		)
	else:
		use_markdown_format()
		pdoc_combined(
			CONFIG.module_name,
			output_file=CONFIG.output_dir / "combined" / f"{CONFIG.package_name}.md",
		)

	# convert notebooks if needed
	if CONFIG.notebooks_enabled:
		convert_notebooks()

	# http server if needed
	# --------------------------------------------------
	if parsed_args.serve:
		import http.server
		import os
		import socketserver

		port: int = 8000
		os.chdir(CONFIG.output_dir)
		with socketserver.TCPServer(
			("", port),
			http.server.SimpleHTTPRequestHandler,
		) as httpd:
			print(f"Serving at http://localhost:{port}")
			httpd.serve_forever()

``````{ end_of_file="scripts/make/make_docs.py" }

``````{ path="scripts/make/pdoc_markdown2_cli.py"  }
# python project makefile template
# https://github.com/mivanit/python-project-makefile-template
# version: ##[[VERSION]]##
# license: https://creativecommons.org/licenses/by-sa/4.0/

"""CLI to convert markdown files to HTML using pdoc's markdown2.

Usage: python pdoc_markdown2_cli.py <input.md> <output.html> [--safe-mode escape|replace]
"""

from __future__ import annotations

import argparse
from pathlib import Path
from typing import Any

from pdoc.markdown2 import (  # type: ignore[import-untyped,import-not-found] # pyright: ignore[reportMissingImports]
	Markdown,  # pyright: ignore[reportUnknownVariableType]
)


def convert_file(
	input_path: Path,
	output_path: Path,
	safe_mode: str | None = None,
	encoding: str = "utf-8",
) -> None:
	"""Convert a markdown file to HTML"""
	# Read markdown input
	text: str = input_path.read_text(encoding=encoding)

	# Convert to HTML using markdown2
	markdown: Any = Markdown(  # pyright: ignore[reportUnknownVariableType]
		extras=["fenced-code-blocks", "header-ids", "markdown-in-html", "tables"],
		safe_mode=safe_mode,
	)
	html: str = str(markdown.convert(text))  # pyright: ignore[reportUnknownMemberType,reportUnknownArgumentType]

	# Write HTML output
	output_path.write_text(html, encoding=encoding)


def main() -> None:
	"cli entry point"
	parser: argparse.ArgumentParser = argparse.ArgumentParser(
		description="Convert markdown files to HTML using pdoc's markdown2",
	)
	parser.add_argument("input", type=Path, help="Input markdown file path")
	parser.add_argument("output", type=Path, help="Output HTML file path")
	parser.add_argument(
		"--safe-mode",
		choices=["escape", "replace"],
		help="Sanitize literal HTML: 'escape' escapes HTML meta chars, 'replace' replaces with [HTML_REMOVED]",
	)
	parser.add_argument(
		"--encoding",
		default="utf-8",
		help="Character encoding for reading/writing files (default: utf-8)",
	)

	args: argparse.Namespace = parser.parse_args()

	convert_file(
		args.input,
		args.output,
		safe_mode=args.safe_mode,
		encoding=args.encoding,
	)


if __name__ == "__main__":
	main()

``````{ end_of_file="scripts/make/pdoc_markdown2_cli.py" }

``````{ path="scripts/make/recipe_info.py"  }
# python project makefile template
# https://github.com/mivanit/python-project-makefile-template
# version: ##[[VERSION]]##
# license: https://creativecommons.org/licenses/by-sa/4.0/

"""CLI tool to get information about Makefile recipes/targets.

Parses a makefile and displays detailed info about targets including
dependencies, comments, and echo messages. Supports fuzzy matching
and wildcard patterns.

Usage: python recipe_info.py [-f makefile] [--all | target1 target2 ...]
"""

from __future__ import annotations

import argparse
import difflib
import fnmatch
import re
import sys
from dataclasses import dataclass
from pathlib import Path
from typing import Literal, cast, overload


@overload
def _scan_makefile(
	lines: list[str],
	target_name: str,
) -> int: ...
@overload
def _scan_makefile(
	lines: list[str],
	target_name: None = None,
) -> dict[str, int]: ...
def _scan_makefile(
	lines: list[str],
	target_name: str | None = None,
) -> dict[str, int] | int:
	"""Scan makefile for target definitions, skipping define blocks.

	Args:
		lines: Makefile lines
		target_name: If provided, return line index for this specific target.
					If None, return dict of all targets.

	Returns:
		If target_name is None: dict mapping target names to line indices
		If target_name is provided: line index of that target, or -1 if not found

	"""
	in_define_block: bool = False
	target_rx: re.Pattern[str] = re.compile(r"^([a-zA-Z0-9_-]+)[ \t]*:")
	targets: dict[str, int] = {}

	for i, line in enumerate(lines):
		# Track if we're inside a define block (embedded scripts)
		if line.startswith("define "):
			in_define_block = True
			continue
		if line.startswith("endef"):
			in_define_block = False
			continue

		# Skip lines inside define blocks
		if in_define_block:
			continue

		# Match target definitions
		match = target_rx.match(line)
		if match:
			tgt_name: str = match.group(1)
			if target_name is not None:
				# Looking for specific target
				if tgt_name == target_name:
					return i
			else:
				# Collecting all targets
				targets[tgt_name] = i

	# Return results based on mode
	if target_name is not None:
		return -1  # Target not found
	return targets


def _scan_makefile_variables(lines: list[str]) -> dict[str, int]:
	"""Scan makefile for variable definitions, skipping define blocks.

	Returns dict mapping variable names to line indices.
	Only matches variables that start with uppercase letter or underscore.
	"""
	in_define_block: bool = False
	# Match: VARNAME := value, VARNAME ?= value, VARNAME += value, VARNAME = value
	# Allow leading whitespace to match variables inside ifeq/endif blocks
	var_rx: re.Pattern[str] = re.compile(
		r"^\s*([A-Z_][A-Z0-9_]*)\s*(\?=|:=|\+=|=)\s*(.*)$"
	)
	variables: dict[str, int] = {}

	for i, line in enumerate(lines):
		# Track if we're inside a define block
		if line.startswith("define "):
			in_define_block = True
			continue
		if line.startswith("endef"):
			in_define_block = False
			continue

		# Skip lines inside define blocks
		if in_define_block:
			continue

		# Match variable definitions
		match = var_rx.match(line)
		if match:
			var_name: str = match.group(1)
			# Only store the first definition (in case of multiple assignments)
			if var_name not in variables:
				variables[var_name] = i

	return variables


class Colors:
	"""ANSI color codes"""

	RESET: str
	BOLD: str
	RED: str
	GREEN: str
	YELLOW: str
	BLUE: str
	MAGENTA: str
	CYAN: str
	WHITE: str

	def __init__(self, enabled: bool = True) -> None:
		"init color codes, or empty strings if `not enabled`"
		if enabled:
			self.RESET = "\033[0m"  # pyright: ignore[reportConstantRedefinition]
			self.BOLD = "\033[1m"  # pyright: ignore[reportConstantRedefinition]
			self.RED = "\033[31m"  # pyright: ignore[reportConstantRedefinition]
			self.GREEN = "\033[32m"  # pyright: ignore[reportConstantRedefinition]
			self.YELLOW = "\033[33m"  # pyright: ignore[reportConstantRedefinition]
			self.BLUE = "\033[34m"  # pyright: ignore[reportConstantRedefinition]
			self.MAGENTA = "\033[35m"  # pyright: ignore[reportConstantRedefinition]
			self.CYAN = "\033[36m"  # pyright: ignore[reportConstantRedefinition]
			self.WHITE = "\033[37m"  # pyright: ignore[reportConstantRedefinition]
		else:
			self.RESET = self.BOLD = ""  # pyright: ignore[reportConstantRedefinition]
			self.RED = self.GREEN = self.YELLOW = ""  # pyright: ignore[reportConstantRedefinition]
			self.BLUE = self.MAGENTA = self.CYAN = self.WHITE = ""  # pyright: ignore[reportConstantRedefinition]


@dataclass
class MakeRecipe:
	"""Information about a Makefile recipe/target."""

	target: str
	comments: list[str]
	dependencies: list[str]
	echo_message: str

	@classmethod
	def from_makefile(cls, lines: list[str], target: str) -> MakeRecipe:
		"""Parse and create a MakeRecipe from makefile lines for *target*."""
		i: int = _scan_makefile(lines, target_name=target)
		if i == -1:
			err_msg: str = f"target '{target}' not found in makefile"
			raise ValueError(err_msg)

		line: str = lines[i]

		# contiguous comment block immediately above
		# (skip backward past .PHONY declarations and blank lines)
		comments: list[str] = []
		j: int = i - 1
		blank_count: int = 0
		stripped: str
		while j >= 0:
			stripped = lines[j].lstrip()
			if stripped.startswith("#"):
				comments.append(stripped[1:].lstrip())
				blank_count = 0  # Reset blank counter when we hit a comment
				j -= 1
			elif stripped == "":
				# Track consecutive blank lines
				blank_count += 1
				if blank_count >= 2:
					# Hit 2 blank lines in a row - stop
					break
				j -= 1
			elif stripped.startswith(".PHONY:"):
				# Skip .PHONY declarations
				blank_count = 0  # Reset blank counter
				j -= 1
			else:
				# Hit a non-comment, non-blank, non-.PHONY line - stop
				break
		comments.reverse()

		# prerequisites
		deps_str: str = line.split(":", 1)[1].strip()
		deps: list[str] = deps_str.split() if deps_str else []

		# first echo in the recipe
		echo_msg: str = ""
		k: int = i + 1
		while k < len(lines) and (
			lines[k].startswith("\t") or lines[k].startswith("    ")
		):
			m = re.match(r"@?echo[ \t]+(.*)", lines[k].lstrip())
			if m:
				content: str = m.group(1).strip()
				if (content.startswith('"') and content.endswith('"')) or (
					content.startswith("'") and content.endswith("'")
				):
					content = content[1:-1]
				echo_msg = content
				break
			k += 1

		return cls(
			target=target,
			comments=comments,
			dependencies=deps,
			echo_message=echo_msg,
		)

	def describe(self, color: bool = False) -> list[str]:
		"""Return a list of description lines for this recipe."""
		output: list[str] = []
		c: Colors = Colors(enabled=color)

		# Target name (bold blue) with colon in white
		output.append(f"{c.BOLD}{c.BLUE}{self.target}{c.RESET}{c.WHITE}:{c.RESET}")

		# Echo message (description) in yellow
		if self.echo_message:
			output.append(f"  {c.YELLOW}{self.echo_message}{c.RESET}")

		# Dependencies in magenta
		if self.dependencies:
			deps_str = " ".join(
				f"{c.MAGENTA}{dep}{c.RESET}" for dep in self.dependencies
			)
			output.append(f"  {c.RED}depends-on:{c.RESET} {deps_str}")

		# Comments in green
		if self.comments:
			output.append(f"  {c.RED}comments:{c.RESET}")
			output.extend(f"    {c.GREEN}{line}{c.RESET}" for line in self.comments)

		return output


@dataclass
class MakeVariable:
	"""Information about a Makefile variable."""

	name: str
	raw_value: str  # as written in makefile, e.g., "$(shell git describe)"
	operator: Literal["=", ":=", "?=", "+="]
	comments: list[str]  # comments above the definition

	@classmethod
	def from_makefile(
		cls,
		lines: list[str],
		var_name: str,
		var_line_idx: int,
	) -> MakeVariable:
		"""Parse and create a MakeVariable from makefile lines."""
		line: str = lines[var_line_idx]

		# Parse the variable definition line (allow leading whitespace for ifeq blocks)
		var_rx: re.Pattern[str] = re.compile(
			r"^\s*([A-Z_][A-Z0-9_]*)\s*(\?=|:=|\+=|=)\s*(.*)$"
		)
		match = var_rx.match(line)
		if not match:
			err_msg: str = f"variable '{var_name}' not found at line {var_line_idx}"
			raise ValueError(err_msg)

		operator: Literal["=", ":=", "?=", "+="] = match.group(2)  # type: ignore[assignment] # pyright: ignore[reportAssignmentType]
		raw_value: str = match.group(3)

		# Collect contiguous comment block above (same logic as MakeRecipe)
		comments: list[str] = []
		j: int = var_line_idx - 1
		blank_count: int = 0
		while j >= 0:
			stripped: str = lines[j].lstrip()
			if stripped.startswith("#"):
				comments.append(stripped[1:].lstrip())
				blank_count = 0
				j -= 1
			elif stripped == "":
				blank_count += 1
				if blank_count >= 2:
					break
				j -= 1
			else:
				break
		comments.reverse()

		return cls(
			name=var_name,
			raw_value=raw_value,
			operator=operator,
			comments=comments,
		)

	def describe(self, color: bool = True) -> list[str]:
		"""Return a list of description lines for this variable."""
		output: list[str] = []
		c: Colors = Colors(enabled=color)

		# Variable name in bold cyan, operator in white
		output.append(
			f"{c.BOLD}{c.CYAN}{self.name}{c.RESET} {c.WHITE}{self.operator}{c.RESET}"
		)

		# Raw value in yellow
		output.append(f"  {c.YELLOW}{self.raw_value}{c.RESET}")

		# Comments in green (same style as targets)
		if self.comments:
			output.append(f"  {c.RED}comments:{c.RESET}")
			output.extend(f"    {c.GREEN}{line}{c.RESET}" for line in self.comments)

		# Hint for computed values
		output.append(f"  {c.WHITE}(run 'make info-long' for computed values){c.RESET}")

		return output


def find_all_variables(lines: list[str]) -> dict[str, int]:
	"""Find all variable definitions in the makefile.

	Returns dict mapping variable names to line indices.
	"""
	return _scan_makefile_variables(lines)


def find_all_targets(lines: list[str]) -> list[str]:
	"""Find all .PHONY target names in the makefile."""
	# First, get all .PHONY declarations
	phony_targets: set[str] = set()
	# Use chr(36) to get dollar sign - works both standalone and embedded in makefile
	# issue being that the makefile processes dollar sign as an escape character
	phony_pattern: re.Pattern[str] = re.compile(r"^\.PHONY:\s+(.+)" + chr(36))

	for line in lines:
		match = phony_pattern.match(line)
		if match:
			# Get all targets from this .PHONY line (space-separated)
			target_names: list[str] = match.group(1).split()
			phony_targets.update(target_names)

	# Now scan for actual target definitions and filter to .PHONY ones
	all_target_defs: dict[str, int] = _scan_makefile(lines)
	return [tgt for tgt in all_target_defs if tgt in phony_targets]


def get_all_recipes(lines: list[str]) -> list[MakeRecipe]:
	"""Get MakeRecipe objects for all .PHONY targets in the makefile."""
	targets: list[str] = find_all_targets(lines)
	return [MakeRecipe.from_makefile(lines, target) for target in targets]


def describe_target(makefile_path: Path, target: str) -> None:
	"""Emit the description for *target*."""
	lines: list[str] = makefile_path.read_text(encoding="utf-8").splitlines()
	recipe: MakeRecipe = MakeRecipe.from_makefile(lines, target)

	for line in recipe.describe():
		print(line)


def main() -> None:  # noqa: PLR0912, PLR0915, C901
	"""CLI entry point."""
	parser: argparse.ArgumentParser = argparse.ArgumentParser(
		"recipe_info",
		description="Get detailed information about Makefile recipes/targets and variables",
	)
	parser.add_argument(
		"-f",
		"--file",
		default="makefile",
		help="Path to the Makefile (default: ./makefile)",
	)
	parser.add_argument(
		"-a",
		"--all",
		action="store_true",
		help="Print help for all targets in the Makefile",
	)
	parser.add_argument(
		"--no-color",
		action="store_true",
		help="Disable colored output (color is enabled by default)",
	)
	parser.add_argument(
		"targets", nargs="*", help="Target or variable names (case-insensitive)"
	)
	args: argparse.Namespace = parser.parse_args()

	lines: list[str] = Path(args.file).read_text(encoding="utf-8").splitlines()
	c: Colors = Colors(enabled=not args.no_color)

	# Get all targets and variables upfront
	all_targets: list[str] = find_all_targets(lines)
	all_variables: dict[str, int] = find_all_variables(lines)

	recipes: list[MakeRecipe] = []
	variables: list[MakeVariable] = []

	if args.all:
		recipes = get_all_recipes(lines)
	elif args.targets:
		for query in args.targets:
			has_wildcard: bool = any(char in query for char in ["*", "?", "["])

			if has_wildcard:
				# Pattern matching mode for targets
				matched_targets: list[str] = [
					t for t in all_targets if fnmatch.fnmatch(t, query)
				]
				for matched in matched_targets:
					recipes.append(MakeRecipe.from_makefile(lines, matched))

				# Pattern matching for variables (case-insensitive)
				matched_vars: list[str] = [
					v
					for v in all_variables
					if fnmatch.fnmatch(v.lower(), query.lower())
				]

				variables.extend(
					[
						MakeVariable.from_makefile(
							lines=lines,
							var_name=var_name,
							var_line_idx=all_variables[var_name],
						)
						for var_name in matched_vars
					]
				)

				if not matched_targets and not matched_vars:
					print(
						f"Error: no targets or variables match pattern '{c.RED}{query}{c.RESET}'",
						file=sys.stderr,
					)
					sys.exit(1)
			else:
				# Exact/case-insensitive lookup
				found_target: bool = False
				found_variable: bool = False

				# Check for exact target match
				if query in all_targets:
					recipes.append(MakeRecipe.from_makefile(lines, query))
					found_target = True

				# Check for case-insensitive variable match
				query_upper: str = query.upper()
				for var_name, var_line_idx in all_variables.items():
					if var_name.upper() == query_upper:
						variables.append(
							MakeVariable.from_makefile(
								lines=lines,
								var_name=var_name,
								var_line_idx=var_line_idx,
							)
						)
						found_variable = True
						break

				if not found_target and not found_variable:
					# Find similar targets and variables (fuzzy matching)
					all_names: list[str] = all_targets + list(all_variables.keys())
					fuzzy_matches: list[str] = cast(
						"list[str]",
						difflib.get_close_matches(
							query,
							all_names,
							n=5,
							cutoff=0.5,
						),
					)
					# Also find names that contain the query
					substring_matches: list[str] = [
						n
						for n in all_names
						if query.lower() in n.lower() and n not in fuzzy_matches
					]
					matches: list[str] = (fuzzy_matches + substring_matches)[:5]

					print(
						f"Error: '{c.RED}{query}{c.RESET}' not found as target or variable",
						file=sys.stderr,
					)
					if matches:
						suggestions: str = ", ".join(
							f"{c.BLUE}{m}{c.RESET}" for m in matches
						)
						print(f"Did you mean: {suggestions}?", file=sys.stderr)
					sys.exit(1)

	if not recipes and not variables:
		parser.error("Provide target/variable names or use --all flag")

	# Print descriptions
	use_color: bool = not args.no_color
	output_lines: list[str] = []

	# Targets first
	for recipe in recipes:
		output_lines.extend(recipe.describe(color=use_color))

	# Separator if we have both
	if recipes and variables:
		output_lines.append("-" * 40)

	# Variables
	for var in variables:
		output_lines.extend(var.describe(color=use_color))

	print("\n".join(output_lines).replace("\n\n", f"\n{'-' * 40}\n"))


if __name__ == "__main__":
	main()

``````{ end_of_file="scripts/make/recipe_info.py" }

``````{ path="scripts/make/typing_breakdown.py"  }
# python project makefile template
# https://github.com/mivanit/python-project-makefile-template
# version: ##[[VERSION]]##
# license: https://creativecommons.org/licenses/by-sa/4.0/

"""Parse type checker outputs and generate detailed breakdown of errors by type and file.

Usage:
    python typing_breakdown.py [OPTIONS]

Examples:
    python typing_breakdown.py
    python typing_breakdown.py --error-dir .meta/.type-errors
    python typing_breakdown.py --output .meta/typing-summary.toml --checkers mypy,basedpyright,ty

"""

from __future__ import annotations

import argparse
import os
import re
from collections import defaultdict
from dataclasses import dataclass, field
from pathlib import Path
from typing import Callable, Literal


def strip_cwd(path: str) -> str:
	"""Strip the current working directory from a file path to make it relative.

	Args:
		path: File path (absolute or relative)

	Returns:
		Relative path with CWD stripped, or original path if not under CWD

	"""
	cwd: str = os.getcwd()
	# Normalize both paths to handle different separators and resolve symlinks
	abs_path: str = os.path.abspath(path)
	abs_cwd: str = os.path.abspath(cwd)

	# Ensure CWD ends with separator for proper prefix matching
	if not abs_cwd.endswith(os.sep):
		abs_cwd += os.sep

	# Strip CWD prefix if present
	if abs_path.startswith(abs_cwd):
		return abs_path[len(abs_cwd) :]

	return path


@dataclass
class TypeCheckResult:
	"results from parsing a type checker output"

	type_checker: Literal["mypy", "basedpyright", "ty"]
	by_type: dict[str, int] = field(default_factory=lambda: defaultdict(int))
	by_file: dict[str, int] = field(default_factory=lambda: defaultdict(int))
	# Separate tracking for warnings (used by basedpyright)
	warnings_by_type: dict[str, int] = field(default_factory=lambda: defaultdict(int))
	warnings_by_file: dict[str, int] = field(default_factory=lambda: defaultdict(int))

	@property
	def total_errors(self) -> int:
		"total number of errors across all types, validates they match between type and file dicts"
		total_by_type: int = sum(self.by_type.values())
		total_by_file: int = sum(self.by_file.values())

		if total_by_type != total_by_file:
			err_msg: str = f"Error count mismatch for {self.type_checker}: by_type={total_by_type}, by_file={total_by_file}"
			raise ValueError(err_msg)

		return total_by_type

	@property
	def total_warnings(self) -> int:
		"total number of warnings across all types"
		total_by_type: int = sum(self.warnings_by_type.values())
		total_by_file: int = sum(self.warnings_by_file.values())

		if total_by_type != total_by_file:
			err_msg: str = f"Warning count mismatch for {self.type_checker}: by_type={total_by_type}, by_file={total_by_file}"
			raise ValueError(err_msg)

		return total_by_type

	def sorted_results(self) -> TypeCheckResult:
		"return a copy with errors sorted by count (descending)"
		# Sort by count (descending)
		sorted_by_type: list[tuple[str, int]] = sorted(
			self.by_type.items(),
			key=lambda x: x[1],
			reverse=True,
		)
		sorted_by_file: list[tuple[str, int]] = sorted(
			self.by_file.items(),
			key=lambda x: x[1],
			reverse=True,
		)
		sorted_warnings_by_type: list[tuple[str, int]] = sorted(
			self.warnings_by_type.items(),
			key=lambda x: x[1],
			reverse=True,
		)
		sorted_warnings_by_file: list[tuple[str, int]] = sorted(
			self.warnings_by_file.items(),
			key=lambda x: x[1],
			reverse=True,
		)

		# Create new instance with sorted data (dicts maintain insertion order in Python 3.7+)
		result: TypeCheckResult = TypeCheckResult(type_checker=self.type_checker)
		result.by_type = dict(sorted_by_type)
		result.by_file = dict(sorted_by_file)
		result.warnings_by_type = dict(sorted_warnings_by_type)
		result.warnings_by_file = dict(sorted_warnings_by_file)

		return result

	def to_toml(self) -> str:
		"format as TOML output"
		lines: list[str] = []

		# Main section with total
		lines.append(f"[type_errors.{self.type_checker}]")
		try:
			lines.append(f"total_errors = {self.total_errors}")
		except ValueError:
			lines.append(f"total_errors_by_type = {sum(self.by_type.values())}")
			lines.append(f"total_errors_by_file = {sum(self.by_file.values())}")
		lines.append("")

		# by_type section
		lines.append(f"[type_errors.{self.type_checker}.by_type]")
		error_type: str
		count: int
		for error_type, count in self.by_type.items():
			# Always quote keys
			lines.append(f'"{error_type}" = {count}')

		lines.append("")

		# by_file section
		lines.append(f"[type_errors.{self.type_checker}.by_file]")
		file_path: str
		for file_path, count in self.by_file.items():
			# Always quote file paths
			lines.append(f'"{file_path}" = {count}')

		# Add warnings sections if there are any warnings
		if self.warnings_by_type or self.warnings_by_file:
			lines.append("")
			lines.append(f"[type_warnings.{self.type_checker}]")
			try:
				lines.append(f"total_warnings = {self.total_warnings}")
			except ValueError:
				lines.append(
					f"total_warnings_by_type = {sum(self.warnings_by_type.values())}"
				)
				lines.append(
					f"total_warnings_by_file = {sum(self.warnings_by_file.values())}"
				)
			lines.append("")

			# warnings by_type section
			lines.append(f"[type_warnings.{self.type_checker}.by_type]")
			warning_type: str
			for warning_type, count in self.warnings_by_type.items():
				lines.append(f'"{warning_type}" = {count}')

			lines.append("")

			# warnings by_file section
			lines.append(f"[type_warnings.{self.type_checker}.by_file]")
			for file_path, count in self.warnings_by_file.items():
				lines.append(f'"{file_path}" = {count}')

		return "\n".join(lines)


def parse_mypy(content: str) -> TypeCheckResult:
	"parse mypy output: file.py:line: error: message [error-code]"
	result: TypeCheckResult = TypeCheckResult(type_checker="mypy")

	pattern: re.Pattern[str] = re.compile(
		r"^(.+?):\d+: error: .+ \[(.+?)\]", re.MULTILINE
	)
	match: re.Match[str]
	for match in pattern.finditer(content):
		file_path: str = match.group(1)
		error_code: str = match.group(2)
		result.by_type[error_code] += 1
		result.by_file[file_path] += 1

	return result


def parse_basedpyright(content: str) -> TypeCheckResult:
	"parse basedpyright output: path on line, then indented errors with (code)"
	result: TypeCheckResult = TypeCheckResult(type_checker="basedpyright")

	# Pattern for file paths (lines that start with /)
	# Pattern for errors: indented line with - error/warning: message (code)
	# Some diagnostics span multiple lines with (reportCode) on a continuation line
	current_file: str = ""
	pending_diagnostic_type: str | None = None  # "error" or "warning" waiting for code

	line: str
	for line in content.splitlines():
		# Check if this is a file path line (starts with / and no leading space)
		if line and not line.startswith(" ") and line.startswith("/"):
			current_file = strip_cwd(line.strip())
			pending_diagnostic_type = None

		elif line.strip() and current_file:
			# Try to match single-line format: "  path:line:col - warning: message (reportCode)"
			match: re.Match[str] | None = re.search(
				r"\s+.+:\d+:\d+ - (error|warning): .+ \((\w+)\)", line
			)
			if match:
				diagnostic_type: str = match.group(1)
				error_code: str = match.group(2)
				if diagnostic_type == "warning":
					result.warnings_by_type[error_code] += 1
					result.warnings_by_file[current_file] += 1
				else:
					result.by_type[error_code] += 1
					result.by_file[current_file] += 1
				pending_diagnostic_type = None
			else:
				# Check if this is a diagnostic line without code (multi-line format start)
				diag_match: re.Match[str] | None = re.search(
					r"\s+.+:\d+:\d+ - (error|warning): ", line
				)
				if diag_match:
					pending_diagnostic_type = diag_match.group(1)
				# Check if this is a continuation line with the code
				elif pending_diagnostic_type:
					code_match: re.Match[str] | None = re.search(r"\((\w+)\)\s*$", line)
					if code_match:
						error_code = code_match.group(1)
						if pending_diagnostic_type == "warning":
							result.warnings_by_type[error_code] += 1
							result.warnings_by_file[current_file] += 1
						else:
							result.by_type[error_code] += 1
							result.by_file[current_file] += 1
						pending_diagnostic_type = None

	return result


def parse_ty(content: str) -> TypeCheckResult:
	"parse ty output: error[error-code]: message then --> file:line:col"
	result: TypeCheckResult = TypeCheckResult(type_checker="ty")

	# Pattern for error type: error[code]: or warning[code]:
	error_pattern: re.Pattern[str] = re.compile(
		r"^(error|warning)\[(.+?)\]:", re.MULTILINE
	)
	# Pattern for location: --> file:line:col
	location_pattern: re.Pattern[str] = re.compile(
		r"^\s+-->\s+(.+?):\d+:\d+", re.MULTILINE
	)

	# Find all errors and their locations
	errors: list[re.Match[str]] = list(error_pattern.finditer(content))
	locations: list[re.Match[str]] = list(location_pattern.finditer(content))

	# Match errors with locations (they should be in order)
	error_match: re.Match[str]
	for error_match in errors:
		error_code: str = error_match.group(2)
		result.by_type[error_code] += 1

		# Find the next location after this error
		error_pos: int = error_match.end()
		loc_match: re.Match[str]
		for loc_match in locations:
			if loc_match.start() > error_pos:
				file_path: str = loc_match.group(1)
				result.by_file[file_path] += 1
				break

	return result


def extract_summary_line(file_path: Path) -> str:
	"extract the last non-empty line from a file (typically the summary line)"
	content: str = file_path.read_text(encoding="utf-8")
	lines: list[str] = [line.strip() for line in content.splitlines() if line.strip()]
	if not lines:
		return "(empty output)"
	return lines[-1]


def main(error_dir: str, output_file: str, checkers: list[str]) -> None:
	"parse all type checker outputs and generate breakdown"
	error_path: Path = Path(error_dir)
	output_path: Path = Path(output_file)

	output_lines: list[str] = []

	# First, extract summary lines from each type checker
	checkers_info: list[tuple[str, str, Callable[[str], TypeCheckResult]]] = [
		("mypy", "mypy.txt", parse_mypy),
		("basedpyright", "basedpyright.txt", parse_basedpyright),
		("ty", "ty.txt", parse_ty),
	]

	# Add summary comments
	name: str
	filename: str
	for name, filename, _ in checkers_info:
		if name not in checkers:
			continue
		file_path: Path = error_path / filename
		if not file_path.exists():
			output_lines.append(f"# {name}: (not run or file not found)")
		else:
			summary: str = extract_summary_line(file_path)
			output_lines.append(f"# {name}: {summary}")

	output_lines.append("")

	# Parse each type checker
	parser_fn: Callable[[str], TypeCheckResult]
	for name, filename, parser_fn in checkers_info:
		if name not in checkers:
			continue
		file_path_: Path = error_path / filename
		if not file_path_.exists():
			continue
		content: str = file_path_.read_text(encoding="utf-8")
		result: TypeCheckResult = parser_fn(content)
		# Sort the results
		sorted_result: TypeCheckResult = result.sorted_results()
		# Convert to TOML
		breakdown: str = sorted_result.to_toml()
		output_lines.append(breakdown)
		output_lines.append("")  # Add blank line between checkers

	# Write to output file
	final_output: str = "\n".join(output_lines)
	output_path.parent.mkdir(parents=True, exist_ok=True)
	_ = output_path.write_text(final_output, encoding="utf-8")

	# Also print to stdout
	print(final_output)


if __name__ == "__main__":
	parser: argparse.ArgumentParser = argparse.ArgumentParser(
		description="Parse type checker outputs and generate detailed breakdown of errors by type and file",
		formatter_class=argparse.RawDescriptionHelpFormatter,
	)
	_ = parser.add_argument(
		"--error-dir",
		type=str,
		default=".meta/.type-errors",
		help="Directory containing type checker output files (default: .meta/.type-errors)",
	)
	_ = parser.add_argument(
		"--output",
		"-o",
		type=str,
		default=".meta/typing-summary.toml",
		help="Output file to write summary to (default: .meta/typing-summary.toml)",
	)
	_ = parser.add_argument(
		"--checkers",
		"-c",
		type=str,
		default="mypy,basedpyright,ty",
		help="Comma-separated list of checkers to process (default: mypy,basedpyright,ty)",
	)

	args: argparse.Namespace = parser.parse_args()

	# Parse checkers list
	checkers_list: list[str] = [c.strip() for c in args.checkers.split(",")]

	main(error_dir=args.error_dir, output_file=args.output, checkers=checkers_list)

``````{ end_of_file="scripts/make/typing_breakdown.py" }

``````{ path="scripts/assemble.py"  }
# python project makefile template
# https://github.com/mivanit/python-project-makefile-template
# license: https://creativecommons.org/licenses/by-sa/4.0/

"""Assemble the makefile and helper scripts by replacing version placeholders.

Reads version from pyproject.toml and replaces ##[[VERSION]]## placeholders in:
- makefile.template -> makefile
- scripts/make/*.py -> scripts/out/*.py, .meta/scripts/*.py
"""

from __future__ import annotations

from pathlib import Path

try:
	import tomllib  # type: ignore[import-not-found]
except ImportError:
	import tomli as tomllib  # type: ignore

MAKEFILE_TEMPLATE_PATH: Path = Path("makefile.template")
"path of the makefile template which we read"

MAKEFILE_PATH: Path = Path("makefile")
"path of the assembled makefile which we write"

SCRIPTS_DIR: Path = Path("scripts")
"path to all scripts, including this one"

SCRIPTS_MAKE_DIR: Path = SCRIPTS_DIR / "make"
"path to template scripts (with version placeholders)"

SCRIPTS_OUT_DIR: Path = SCRIPTS_DIR / "out"
"path to assembled scripts (with version replaced)"

META_SCRIPTS_DIR: Path = Path(".meta") / "scripts"
"path to meta scripts (used by makefile)"

TEMPLATE_SYNTAX: str = "##[[{var}]]##"
"template syntax in the makefile and script templates"

with open("pyproject.toml", "rb") as f_pyproject:
	VERSION: str = tomllib.load(f_pyproject)["project"]["version"]


def assemble_make() -> None:
	"assemble the makefile (just version replacement)"
	contents: str = MAKEFILE_TEMPLATE_PATH.read_text()

	# version
	version_str: str = f"#| version: v{VERSION}"
	contents = contents.replace(
		TEMPLATE_SYNTAX.format(var="VERSION"),
		f"{version_str:<68}|",
	)

	MAKEFILE_PATH.write_text(contents)


def assemble_scripts() -> None:
	"read template scripts from scripts/make/, replace version, write to scripts/out/ and .meta/scripts/"
	SCRIPTS_OUT_DIR.mkdir(exist_ok=True)
	META_SCRIPTS_DIR.mkdir(parents=True, exist_ok=True)
	for script_path in SCRIPTS_MAKE_DIR.glob("*.py"):
		contents: str = script_path.read_text()
		contents = contents.replace(
			TEMPLATE_SYNTAX.format(var="VERSION"),
			VERSION,
		)
		(SCRIPTS_OUT_DIR / script_path.name).write_text(contents)
		(META_SCRIPTS_DIR / script_path.name).write_text(contents)


if __name__ == "__main__":
	assemble_make()
	assemble_scripts()

``````{ end_of_file="scripts/assemble.py" }

``````{ path="tests/test_nothing.py"  }
def test_nothing():
	print("all good :)")

``````{ end_of_file="tests/test_nothing.py" }

``````{ path="README.md"  }
# Makefile Template for Python Projects

I've ended up using the same style of makefile for multiple Python projects, so I've decided to create a repository with a template.

Relevant ideological decisions:

- **everything contained in github actions should be minimal, and mostly consist of calling makefile recipes**
- [`uv`](https://docs.astral.sh/uv/) for dependency management and packaging
- [`pytest`](https://docs.pytest.org) for testing
- [`mypy`](https://github.com/python/mypy) for static type checking
  - TODO: switch to [`ty`](https://github.com/astral-sh/ty) once it's more mature
- [`ruff`](https://docs.astral.sh/ruff/) for formatting
- [`pdoc`](https://pdoc.dev) for documentation generation
- [`make`](https://en.wikipedia.org/wiki/Make_(software)) for automation
  - I know there are better build tools out there and it's overkill, but `make` is universal. you can think of this as a bunch of hacky additions to `make` to make it a tad more like a modern build tool for python projects
- [`git`](https://github.com/git) for version control (a spicy take, I know)

The whole idea behind this is rather than having a bunch of stuff in your readme describing what commands you need to run to do X, you have those commands in your makefile -- rather than just being human-readable, they are machine-readable.

# How to use this:

- `make` should already be on your system, unless you are on windows
  - I recommend using [gitforwindows.org](https://gitforwindows.org), or just using WSL
- you will need [uv](https://docs.astral.sh/uv/) and some form of python installed.
- run `uv init` or otherwise set up a `pyproject.toml` file
  - the `pyproject.toml` of this repo has dev dependencies that you might need, you may want to copy those
  - it's also got some configuration that is worth looking at
- copy `makefile` from this repo into the root of your repo
- run `make setup` to download helper scripts and sync dependencies
- modify `PACKAGE_NAME := myproject` at the top of the makefile to match your package name
  - there are also a variety of other variables you can modify -- most are at the top of the makefile
- if you want automatic documentation generation, copy `docs/resources/`. it contains:
  - `docs/resources/templates/`: jinja2 templates for the docs, template for the todolist
  - `docs/resources/css/`, `docs/resources/svg/`: some css and icons for the docs


# docs

you can see the generated docs for this repo at [`miv.name/python-project-makefile-template`](https://miv.name/python-project-makefile-template), or the generated docs for the notebooks at [`miv.name/python-project-makefile-template/notebooks`](https://miv.name/python-project-makefile-template/notebooks)

# Makefile

## General Help

`make help` Displays the help message listing all available make targets and variables. Running just `make` will also display this message.

```sh
$ make help
# make targets:
    make build                build the package
    make check                run format checks, tests, and typing checks
    make clean                clean up temporary files
    make clean-all            clean up all temporary files, dep files, venv, and generated docs
    make cov                  generate coverage reports
    make dep                  Exporting dependencies as per $(PYPROJECT) section 'tool.uv-exports.exports'
    make dep-check            Checking that exported requirements are up to date
    make dep-check-torch      see if torch is installed, and which CUDA version and devices it sees
    make dep-clean            clean up lock files, .venv, and requirements files
    make docs                 generate all documentation and coverage reports
    make docs-clean           remove generated docs
    make docs-combined        generate combined (single-file) docs in markdown and convert to other formats
    make docs-html            generate html docs
    make docs-md              generate combined (single-file) docs in markdown
    make format               format the source code
    make format-check         check if the source code is formatted correctly
    make help
    make info                 # makefile variables
    make info-long            # other variables
    make lmcat                write the lmcat full output to pyproject.toml:[tool.lmcat.output]
    make lmcat-tree           show in console the lmcat tree view
    make publish              run all checks, build, and then publish
    make self-setup-scripts  downloading makefile scripts from GitHub
    make setup                install and update via uv
    make test                 running tests
    make todo                 get all TODO's from the code
    make typing               running type checks
    make verify-git           checking git status
    make version              Current version is $(PROJ_VERSION), last auto-uploaded version is $(LAST_VERSION)
# makefile variables
    PYTHON = uv run python
    PYTHON_VERSION = 3.12.0 
    PACKAGE_NAME = myproject
    PROJ_VERSION = v0.0.6 
    LAST_VERSION = v0.0.5
    PYTEST_OPTIONS =  --cov=.
```

## Detailed Help for Specific Targets

You can get detailed information about specific make targets using the `help` variable:

```sh
# Get detailed info about a single target
$ make help=test
test:
  running tests
  depends-on: clean

# Get info about multiple targets
$ make HELP="test clean"
test:
  running tests
  depends-on: clean
clean:
  clean up temporary files
  comments:
    cleans up temp files from formatter, type checking, tests, coverage
    removes all built files
    removes $(TESTS_TEMP_DIR) to remove temporary test files
    ...

# Get info about all targets (wildcard expansion)
$ make h=*
# or
$ make H=--all

# Pattern matching - all targets starting with "dep"
$ make help="dep*"
dep-check-torch:
  see if torch is installed, and which CUDA version and devices it sees
dep:
  Exporting dependencies as per $(PYPROJECT) section 'tool.uv-exports.exports'
dep-check:
  Checking that exported requirements are up to date
dep-clean:
  clean up lock files, .venv, and requirements files
```

All these variations work:
- `make help=TARGET` or `make HELP=TARGET`
- `make h=TARGET` or `make H=TARGET`
- `make help="TARGET1 TARGET2"` (multiple targets)
- `make help=*` or `make h=--all` (all targets)
- `make help="dep*"` (pattern matching with wildcards)
- `make HELP="*clean"` (any target ending in "clean")

Pattern matching supports shell-style wildcards:
- `*` - matches any characters
- `?` - matches any single character
- `[abc]` - matches any character in brackets


# Development

`makefile.template` is the template file for the makefile. Helper scripts are in `scripts/make/` and are downloaded from GitHub via `make self-setup-scripts`.

If developing, modify `makefile.template` or scripts in `scripts/make/`, then run:
```sh
python scripts/assemble.py
```
``````{ end_of_file="README.md" }

``````{ path="makefile"  }
#|==================================================================|
#| python project makefile template                                 |
#| originally by Michael Ivanitskiy (mivanits@umich.edu)            |
#| https://github.com/mivanit/python-project-makefile-template      |
#| version: v0.4.0                                                  |
#| license: https://creativecommons.org/licenses/by-sa/4.0/         |
#|==================================================================|
#| CUSTOMIZATION:                                                   |
#| - modify PACKAGE_NAME and other variables in config section      |
#| - mark custom changes with `~~~~~` for easier template updates   |
#| - run `make help` to see available targets                       |
#| - run `make help=TARGET` for detailed info about specific target |
#|==================================================================|


 ######  ########  ######
##    ## ##       ##    ##
##       ##       ##
##       ######   ##   ####
##       ##       ##    ##
##    ## ##       ##    ##
 ######  ##        ######

# ==================================================
# configuration & variables
# ==================================================


# !!! MODIFY AT LEAST THIS PART TO SUIT YOUR PROJECT !!!
# it assumes that the source is in a directory named the same as the package name
# this also gets passed to some other places
PACKAGE_NAME := myproject

# for checking you are on the right branch when publishing
PUBLISH_BRANCH := main

# where to put docs
# if you change this, you must also change pyproject.toml:tool.makefile.docs.output_dir to match
DOCS_DIR := docs

# where the tests are, for pytest
TESTS_DIR := tests

# tests temp directory to clean up. will remove this in `make clean`
TESTS_TEMP_DIR := $(TESTS_DIR)/_temp/

# probably don't change these:
# --------------------------------------------------

# where the pyproject.toml file is. no idea why you would change this but just in case
PYPROJECT := pyproject.toml

# dir to store various configuration files
# use of `.meta/` inspired by https://news.ycombinator.com/item?id=36472613
META_DIR := .meta

# scripts download configuration
# override SCRIPTS_VERSION to download a specific version (e.g., make self-setup-scripts SCRIPTS_VERSION=v0.5.0)
SCRIPTS_DIR := $(META_DIR)/scripts
SCRIPTS_VERSION ?= main
SCRIPTS_REPO := mivanit/python-project-makefile-template
SCRIPTS_URL_BASE := https://raw.githubusercontent.com/$(SCRIPTS_REPO)/$(SCRIPTS_VERSION)/scripts/out

# requirements.txt files for base package, all extras, dev, and all
REQUIREMENTS_DIR := $(META_DIR)/requirements

# local files (don't push this to git!)
LOCAL_DIR := $(META_DIR)/local

# will print this token when publishing. make sure not to commit this file!!!
PYPI_TOKEN_FILE := $(LOCAL_DIR)/.pypi-token

# version files
VERSIONS_DIR := $(META_DIR)/versions

# the last version that was auto-uploaded. will use this to create a commit log for version tag
# see `gen-commit-log` target
LAST_VERSION_FILE := $(VERSIONS_DIR)/.lastversion

# current version (writing to file needed due to shell escaping issues)
VERSION_FILE := $(VERSIONS_DIR)/.version

# base python to use. Will add `uv run` in front of this if `RUN_GLOBAL` is not set to 1
PYTHON_BASE := python

# where the commit log will be stored
COMMIT_LOG_FILE := $(LOCAL_DIR)/.commit_log

# where to put the coverage reports
# note that this will be published with the docs!
# modify the `docs` targets and `.gitignore` if you don't want that
COVERAGE_REPORTS_DIR := $(DOCS_DIR)/coverage

# this stuff in the docs will be kept
# in addition to anything specified in `pyproject.toml:tool.makefile.docs.no_clean`
DOCS_RESOURCES_DIR := $(DOCS_DIR)/resources

# location of the make docs script
MAKE_DOCS_SCRIPT_PATH := $(SCRIPTS_DIR)/make_docs.py

# version vars - extracted automatically from `pyproject.toml`, `$(LAST_VERSION_FILE)`, and $(PYTHON)
# --------------------------------------------------

# assuming your `pyproject.toml` has a line that looks like `version = "0.0.1"`, `gen-version-info` will extract this
PROJ_VERSION := NULL
# `gen-version-info` will read the last version from `$(LAST_VERSION_FILE)`, or `NULL` if it doesn't exist
LAST_VERSION := NULL
# get the python version, now that we have picked the python command
PYTHON_VERSION := NULL


# ==================================================
# reading command line options
# ==================================================


# for formatting or something, we might want to run python without uv
# RUN_GLOBAL=1 to use global `PYTHON_BASE` instead of `uv run $(PYTHON_BASE)`
RUN_GLOBAL ?= 0

# for running tests or other commands without updating the env, set this to 1
# and it will pass `--no-sync` to `uv run`
UV_NOSYNC ?= 0

ifeq ($(RUN_GLOBAL),0)
	ifeq ($(UV_NOSYNC),1)
		PYTHON = uv run --no-sync $(PYTHON_BASE)
	else
		PYTHON = uv run $(PYTHON_BASE)
	endif
else
	PYTHON = $(PYTHON_BASE)
endif

# if you want different behavior for different python versions
# --------------------------------------------------
# COMPATIBILITY_MODE := $(shell $(PYTHON) -c "import sys; print(1 if sys.version_info < (3, 10) else 0)")

# options we might want to pass to pytest
# --------------------------------------------------

# base options for pytest, user can set this when running make to add more options
PYTEST_OPTIONS ?=

# type checker configuration
# --------------------------------------------------

# which type checkers to run (comma-separated)
# available: mypy,basedpyright,ty
TYPE_CHECKERS ?= mypy,basedpyright,ty

# path to type check (empty = use config from pyproject.toml)
TYPECHECK_PATH ?=

# directory to store type checker outputs
TYPE_ERRORS_DIR := $(META_DIR)/.type-errors

# typing summary output file
TYPING_SUMMARY_FILE := $(META_DIR)/typing-summary.toml

# ==================================================
# default target (help)
# ==================================================


# first/default target is help
.PHONY: default
default: help



 ######   ######  ########  #### ########  ########  ######
##    ## ##    ## ##     ##  ##  ##     ##    ##    ##    ##
##       ##       ##     ##  ##  ##     ##    ##    ##
 ######  ##       ########   ##  ########     ##     ######
      ## ##       ##   ##    ##  ##           ##          ##
##    ## ##    ## ##    ##   ##  ##           ##    ##    ##
 ######   ######  ##     ## #### ##           ##     ######

# ==================================================
# downloading scripts from github
# ==================================================

# list of scripts to download when running `make self-setup-scripts`. these are the helper scripts that the makefile uses for various tasks (e.g., getting version info, generating docs, etc.)
SCRIPTS_LIST := export_requirements get_version get_commit_log check_torch get_todos pdoc_markdown2_cli docs_clean typing_breakdown recipe_info make_docs generate_badge

# download makefile helper scripts from GitHub
# uses curl to fetch scripts from the template repository
# override version: make self-setup-scripts SCRIPTS_VERSION=v0.5.0
.PHONY: self-setup-scripts
self-setup-scripts:
	@echo "downloading makefile scripts (version: $(SCRIPTS_VERSION))"
	@mkdir -p $(SCRIPTS_DIR)
	@for script in $(SCRIPTS_LIST); do \
		echo "  $$script.py"; \
		curl -fsSL "$(SCRIPTS_URL_BASE)/$$script.py" -o "$(SCRIPTS_DIR)/$$script.py"; \
	done
	@echo "$(SCRIPTS_VERSION)" > $(SCRIPTS_DIR)/VERSION
	@echo "done"

##     ## ######## ########   ######  ####  #######  ##    ##
##     ## ##       ##     ## ##    ##  ##  ##     ## ###   ##
##     ## ##       ##     ## ##        ##  ##     ## ####  ##
##     ## ######   ########   ######   ##  ##     ## ## ## ##
 ##   ##  ##       ##   ##         ##  ##  ##     ## ##  ####
  ## ##   ##       ##    ##  ##    ##  ##  ##     ## ##   ###
   ###    ######## ##     ##  ######  ####  #######  ##    ##

# ==================================================
# getting version info
# we do this in a separate target because it takes a bit of time
# ==================================================


# this recipe is weird. we need it because:
# - a one liner for getting the version with toml is unwieldy, and using regex is fragile
# - using $$SCRIPT_GET_VERSION within $(shell ...) doesn't work because of escaping issues
# - trying to write to the file inside the `gen-version-info` recipe doesn't work, 
#   shell eval happens before our `python ...` gets run and `cat` doesn't see the new file
.PHONY: write-proj-version
write-proj-version:
	@mkdir -p $(VERSIONS_DIR)
	@$(PYTHON) $(SCRIPTS_DIR)/get_version.py "$(PYPROJECT)" > $(VERSION_FILE)

# gets version info from $(PYPROJECT), last version from $(LAST_VERSION_FILE), and python version
# uses just `python` for everything except getting the python version. no echo here, because this is "private"
.PHONY: gen-version-info
gen-version-info: write-proj-version
	@mkdir -p $(LOCAL_DIR)
	$(eval PROJ_VERSION := $(shell cat $(VERSION_FILE)) )
	$(eval LAST_VERSION := $(shell [ -f $(LAST_VERSION_FILE) ] && cat $(LAST_VERSION_FILE) || echo NULL) )
	$(eval PYTHON_VERSION := $(shell $(PYTHON) -c "import sys; print(f'{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}')") )

# getting commit log since the tag specified in $(LAST_VERSION_FILE)
# will write to $(COMMIT_LOG_FILE)
# when publishing, the contents of $(COMMIT_LOG_FILE) will be used as the tag description (but can be edited during the process)
# no echo here, because this is "private"
.PHONY: gen-commit-log
gen-commit-log: gen-version-info
	@if [ "$(LAST_VERSION)" = "NULL" ]; then \
		echo "!!! ERROR !!!"; \
		echo "LAST_VERSION is NULL, cant get commit log!"; \
		exit 1; \
	fi
	@mkdir -p $(LOCAL_DIR)
	@$(PYTHON) $(SCRIPTS_DIR)/get_commit_log.py "$(LAST_VERSION)" "$(COMMIT_LOG_FILE)"


# force the version info to be read, printing it out
# also force the commit log to be generated, and cat it out
.PHONY: version
version: gen-commit-log
	@echo "Current version is $(PROJ_VERSION), last auto-uploaded version is $(LAST_VERSION)"
	@echo "Commit log since last version from '$(COMMIT_LOG_FILE)':"
	@cat $(COMMIT_LOG_FILE)
	@echo ""
	@if [ "$(PROJ_VERSION)" = "$(LAST_VERSION)" ]; then \
		echo "!!! ERROR !!!"; \
		echo "Python package $(PROJ_VERSION) is the same as last published version $(LAST_VERSION), exiting!"; \
		exit 1; \
	fi



########  ######## ########   ######
##     ## ##       ##     ## ##    ##
##     ## ##       ##     ## ##
##     ## ######   ########   ######
##     ## ##       ##              ##
##     ## ##       ##        ##    ##
########  ######## ##         ######

# ==================================================
# dependencies and setup
# ==================================================


.PHONY: setup
setup: self-setup-scripts dep-check
	@echo "download scripts and sync dependencies"
	@echo ""
	@echo "setup complete! To activate the virtual environment, run one of:"
	@echo "  source .venv/bin/activate"
	@echo "  source .venv/Scripts/activate"

.PHONY: dep-check-torch
dep-check-torch:
	@echo "see if torch is installed, and which CUDA version and devices it sees"
	$(PYTHON) $(SCRIPTS_DIR)/check_torch.py

# sync dependencies and export to requirements.txt files
# - syncs all extras and groups with uv (including dev dependencies)
# - compiles bytecode for faster imports
# - exports to requirements.txt files per tool.uv-exports.exports config
# configure via pyproject.toml:[tool.uv-exports]:
#   [tool.uv-exports]
#   exports = [
#     { name = "base", extras = [], groups = [] },  # base package deps only
#     { name = "dev", extras = [], groups = ["dev"] },  # dev dependencies
#     { name = "all", extras = ["all"], groups = ["dev"] }  # everything
#   ]
.PHONY: dep
dep:
	@echo "Exporting dependencies as per $(PYPROJECT) section 'tool.uv-exports.exports'"
	uv sync --all-extras --all-groups --compile-bytecode
	mkdir -p $(REQUIREMENTS_DIR)
	$(PYTHON) $(SCRIPTS_DIR)/export_requirements.py $(PYPROJECT) $(REQUIREMENTS_DIR) | sh -x


# verify that requirements.txt files match current dependencies
# - exports deps to temp directory
# - diffs temp against existing requirements files
# - FAILS if any differences found (means you need to run `make dep`)
# useful in CI to catch when pyproject.toml changed but requirements weren't regenerated
.PHONY: dep-check
dep-check:
	@echo "Checking that exported requirements are up to date"
	uv sync --all-extras --all-groups
	mkdir -p $(REQUIREMENTS_DIR)-TEMP
	$(PYTHON) $(SCRIPTS_DIR)/export_requirements.py $(PYPROJECT) $(REQUIREMENTS_DIR)-TEMP | sh -x
	diff -r $(REQUIREMENTS_DIR)-TEMP $(REQUIREMENTS_DIR)
	rm -rf $(REQUIREMENTS_DIR)-TEMP


.PHONY: dep-clean
dep-clean:
	@echo "clean up lock files, .venv, and requirements files"
	rm -rf .venv
	rm -rf uv.lock
	rm -rf $(REQUIREMENTS_DIR)/*.txt


 ######  ##     ## ########  ######  ##    ##  ######
##    ## ##     ## ##       ##    ## ##   ##  ##    ##
##       ##     ## ##       ##       ##  ##   ##
##       ######### ######   ##       #####     ######
##       ##     ## ##       ##       ##  ##         ##
##    ## ##     ## ##       ##    ## ##   ##  ##    ##
 ######  ##     ## ########  ######  ##    ##  ######

# ==================================================
# checks (formatting/linting, typing, tests)
# ==================================================


# format code AND auto-fix linting issues
# performs TWO operations: reformats code, then auto-fixes safe linting issues
# configure in pyproject.toml:[tool.ruff]
.PHONY: format
format:
	@echo "format the source code"
	$(PYTHON) -m ruff format --config $(PYPROJECT) .
	$(PYTHON) -m ruff check --fix --config $(PYPROJECT) .

# runs ruff to check if the code is formatted correctly
.PHONY: format-check
format-check:
	@echo "check if the source code is formatted correctly"
	$(PYTHON) -m ruff check --config $(PYPROJECT) .

# runs type checks with configured checkers
# set TYPE_CHECKERS to customize which checkers run (e.g., TYPE_CHECKERS=mypy,basedpyright)
# set TYPING_OUTPUT_DIR to save outputs to files (used by typing-summary)
# returns exit code 1 if any checker fails
.PHONY: typing
typing:
	@echo "running type checks"
	@div="--------------------------------------------------"; \
	failed=0; \
	for c in $$(echo "$(TYPE_CHECKERS)" | tr ',' ' '); do \
		case "$$c" in ty) subcmd="check";; *) subcmd="";; esac; \
		printf "\033[36m$$div\n[$$c]\n$$div\033[0m\n"; \
		$(PYTHON) -m $$c $$subcmd $(TYPECHECK_ARGS) $(TYPECHECK_PATH) $(if $(TYPING_OUTPUT_DIR),> $(TYPING_OUTPUT_DIR)/$$c.txt 2>&1) || failed=1; \
	done; \
	if [ $$failed -eq 1 ]; then \
		printf "\033[31m$$div\nnot all type checks passed\n$$div\033[0m\n"; \
		exit 1; \
	else \
		printf "\033[32m$$div\nall type checks passed\n$$div\033[0m\n"; \
	fi

# save type check outputs and generate detailed breakdown
# outputs are saved to $(TYPE_ERRORS_DIR)/*.txt
# summary is generated to $(TYPING_SUMMARY_FILE)
.PHONY: typing-summary
typing-summary:
	@echo "running type checks and saving to $(TYPE_ERRORS_DIR)/"
	@mkdir -p $(TYPE_ERRORS_DIR)
	-@$(MAKE) --no-print-directory typing TYPING_OUTPUT_DIR=$(TYPE_ERRORS_DIR)
	@echo "generating typing summary..."
	$(PYTHON) $(SCRIPTS_DIR)/typing_breakdown.py --error-dir $(TYPE_ERRORS_DIR) --output $(TYPING_SUMMARY_FILE) --checkers $(TYPE_CHECKERS)

# run tests with pytest
# you can pass custom args. for example:
# make test PYTEST_OPTIONS="--maxfail=1 -x"
# pytest config in pyproject.toml:[tool.pytest.ini_options]
.PHONY: test
test:
	@echo "running tests"
	$(PYTHON) -m pytest $(PYTEST_OPTIONS) $(TESTS_DIR)

.PHONY: check
check: format-check test typing
	@echo "run format checks, tests, and typing checks"


########   #######   ######   ######
##     ## ##     ## ##    ## ##    ##
##     ## ##     ## ##       ##
##     ## ##     ## ##        ######
##     ## ##     ## ##             ##
##     ## ##     ## ##    ## ##    ##
########   #######   ######   ######

# ==================================================
# coverage & docs
# ==================================================


# generates a whole tree of documentation in html format.
# see `$(MAKE_DOCS_SCRIPT_PATH)` and the templates in `$(DOCS_RESOURCES_DIR)/templates/html/` for more info
.PHONY: docs-html
docs-html:
	@echo "generate html docs"
	$(PYTHON) $(MAKE_DOCS_SCRIPT_PATH)

# instead of a whole website, generates a single markdown file with all docs using the templates in `$(DOCS_RESOURCES_DIR)/templates/markdown/`.
# this is useful if you want to have a copy that you can grep/search, but those docs are much messier.
.PHONY: docs-md
docs-md:
	@echo "generate combined (single-file) docs in markdown"
	mkdir $(DOCS_DIR)/combined -p
	$(PYTHON) $(MAKE_DOCS_SCRIPT_PATH) --combined

# generate coverage reports from test results
# WARNING: if .coverage file not found, will automatically run `make test` first
# - generates text report: $(COVERAGE_REPORTS_DIR)/coverage.txt
# - generates SVG badge: $(COVERAGE_REPORTS_DIR)/coverage.svg
# - generates HTML report: $(COVERAGE_REPORTS_DIR)/html/
# - removes .gitignore from html dir (we publish coverage with docs)
.PHONY: cov
cov:
	@echo "generate coverage reports"
	@if [ ! -f .coverage ]; then \
		echo ".coverage not found, running tests first..."; \
		$(MAKE) test PYTEST_OPTIONS="$(PYTEST_OPTIONS) --cov=." ; \
	fi
	mkdir $(COVERAGE_REPORTS_DIR) -p
	$(PYTHON) -m coverage report -m > $(COVERAGE_REPORTS_DIR)/coverage.txt
	$(PYTHON) $(SCRIPTS_DIR)/generate_badge.py --coverage $(COVERAGE_REPORTS_DIR)/coverage.txt > $(COVERAGE_REPORTS_DIR)/coverage.svg
	$(PYTHON) -m coverage html --directory=$(COVERAGE_REPORTS_DIR)/html/
	rm -rf $(COVERAGE_REPORTS_DIR)/html/.gitignore

# runs the coverage report, then the docs, then the combined docs
.PHONY: docs
docs: cov docs-html docs-md todo lmcat
	@echo "generate all documentation and coverage reports"

# remove generated documentation files, but preserve resources
# - removes all docs except those in DOCS_RESOURCES_DIR
# - preserves files/patterns specified in pyproject.toml config
# - distinct from `make clean` (which removes temp build files, not docs)
# configure via pyproject.toml:[tool.makefile.docs]:
#   [tool.makefile.docs]
#   output_dir = "docs"  # must match DOCS_DIR in makefile
#   no_clean = [  # files/patterns to preserve when cleaning
#     "resources/**",
#     "*.svg",
#     "*.css"
#   ]
.PHONY: docs-clean
docs-clean:
	@echo "remove generated docs except resources"
	$(PYTHON) $(SCRIPTS_DIR)/docs_clean.py $(PYPROJECT) $(DOCS_DIR) $(DOCS_RESOURCES_DIR)


# get all TODO's from the code
# configure via pyproject.toml:[tool.makefile.inline-todo]:
#   [tool.makefile.inline-todo]
#   search_dir = "."  # directory to search for TODOs
#   out_file_base = "docs/other/todo-inline"  # output file path (without extension)
#   context_lines = 2  # lines of context around each TODO
#   extensions = ["py", "md"]  # file extensions to search
#   tags = ["CRIT", "TODO", "FIXME", "HACK", "BUG", "DOC"]  # tags to look for
#   exclude = ["docs/**", ".venv/**", "scripts/get_todos.py"]  # patterns to exclude
#   branch = "main"  # git branch for URLs
#   # repo_url = "..."  # repository URL (defaults to [project.urls.{repository,github}])
#   # template_md = "..."  # custom jinja2 template for markdown output
#   # template_issue = "..."  # custom format string for issues
#   # template_html_source = "..."  # custom html template path
#   tag_label_map = { "BUG" = "bug", "TODO" = "enhancement", "DOC" = "documentation" } # mapping of tags to GitHub issue labels
.PHONY: todo
todo:
	@echo "get all TODO's from the code"
	$(PYTHON) $(SCRIPTS_DIR)/get_todos.py

.PHONY: lmcat-tree
lmcat-tree:
	@echo "show in console the lmcat tree view"
	-$(PYTHON) -m lmcat -t --output STDOUT

.PHONY: lmcat
lmcat:
	@echo "write the lmcat full output to pyproject.toml:[tool.lmcat.output]"
	-$(PYTHON) -m lmcat

########  ##     ## #### ##       ########
##     ## ##     ##  ##  ##       ##     ##
##     ## ##     ##  ##  ##       ##     ##
########  ##     ##  ##  ##       ##     ##
##     ## ##     ##  ##  ##       ##     ##
##     ## ##     ##  ##  ##       ##     ##
########   #######  #### ######## ########

# ==================================================
# build and publish
# ==================================================


# verify git is ready for publishing
# REQUIRES:
# - current branch must be $(PUBLISH_BRANCH)
# - no uncommitted changes (git status --porcelain must be empty)
# EXITS with error if either condition fails
.PHONY: verify-git
verify-git:
	@echo "checking git status"
	if [ "$(shell git branch --show-current)" != $(PUBLISH_BRANCH) ]; then \
		echo "!!! ERROR !!!"; \
		echo "Git is not on the $(PUBLISH_BRANCH) branch, exiting!"; \
		git branch; \
		git status; \
		exit 1; \
	fi; \
	if [ -n "$(shell git status --porcelain)" ]; then \
		echo "!!! ERROR !!!"; \
		echo "Git is not clean, exiting!"; \
		git status; \
		exit 1; \
	fi; \


# build package distribution files
# creates wheel (.whl) and source distribution (.tar.gz) in dist/
.PHONY: build
build:
	@echo "build the package"
	uv build

# publish package to PyPI and create git tag
# PREREQUISITES:
# - must be on $(PUBLISH_BRANCH) branch with clean git status (verified by verify-git)
# - must have $(PYPI_TOKEN_FILE) with your PyPI token
# - version in pyproject.toml must be different from $(LAST_VERSION_FILE)
# PROCESS:
# 1. runs checks, validates version, builds package, verifies git clean
# 2. prompts for version confirmation (you can edit $(COMMIT_LOG_FILE) at this point)
# 3. creates git commit updating $(LAST_VERSION_FILE)
# 4. creates annotated git tag with commit log as description
# 5. pushes tag to origin
# 6. uploads to PyPI via twine
.PHONY: publish
publish: check version build verify-git
	@echo "Ready to publish $(PROJ_VERSION) to PyPI"
	@echo "Now would be the time to edit $(COMMIT_LOG_FILE) for the tag description"

	@read -p "Enter version to confirm: " NEW_VERSION && \
	if [ "$$NEW_VERSION" != "$(PROJ_VERSION)" ]; then \
		echo "Version mismatch: got $$NEW_VERSION, expected $(PROJ_VERSION)"; \
		exit 1; \
	fi && \
	echo "Version confirmed."

	@test -f "$(PYPI_TOKEN_FILE)" || { echo "ERROR: Token file not found at $(PYPI_TOKEN_FILE)"; exit 1; }

	@echo "Committing and tagging..."
	echo $(PROJ_VERSION) > $(LAST_VERSION_FILE) && \
	git add $(LAST_VERSION_FILE) && \
	git commit -m "Auto update to $(PROJ_VERSION)" && \
	git tag -a $(PROJ_VERSION) -F $(COMMIT_LOG_FILE) && \
	git push origin $(PROJ_VERSION)

	@echo "Uploading to PyPI..."
	TWINE_USERNAME=__token__ TWINE_PASSWORD="$$(cat $(PYPI_TOKEN_FILE))" $(PYTHON) -m twine upload dist/* --verbose

	@echo "Published $(PROJ_VERSION) successfully!"

# ==================================================
# cleanup of temp files
# ==================================================


# cleans up temporary files:
# - caches: .mypy_cache, .ruff_cache, .pytest_cache, .coverage
# - build artifacts: dist/, build/, *.egg-info
# - test temp files: $(TESTS_TEMP_DIR)
# - __pycache__ directories and *.pyc/*.pyo files in $(PACKAGE_NAME), $(TESTS_DIR), $(DOCS_DIR)
# uses `-` prefix on find commands to continue even if directories don't exist
# distinct from `make docs-clean`, which removes generated documentation
.PHONY: clean
clean:
	@echo "clean up temporary files"
	rm -rf .mypy_cache .ruff_cache .pytest_cache .coverage dist build $(PACKAGE_NAME).egg-info $(TESTS_TEMP_DIR) $(TYPE_ERRORS_DIR)
	-find $(PACKAGE_NAME) $(TESTS_DIR) $(DOCS_DIR) -type d -name '__pycache__' -exec rm -rf {} +
	-find $(PACKAGE_NAME) $(TESTS_DIR) $(DOCS_DIR) -type f -name '*.py[co]' -delete

# remove all generated/build files including .venv
# runs: clean + docs-clean + dep-clean
# removes .venv, uv.lock, requirements.txt files, generated docs, build artifacts
# run `make dep` after this to reinstall dependencies
.PHONY: clean-all
clean-all: clean docs-clean dep-clean
	@echo "clean up all temporary files, dep files, venv, and generated docs"


##     ## ######## ##       ########
##     ## ##       ##       ##     ##
##     ## ##       ##       ##     ##
######### ######   ##       ########
##     ## ##       ##       ##
##     ## ##       ##       ##
##     ## ######## ######## ##

# ==================================================
# smart help command
# ==================================================


# listing targets is from stackoverflow
# https://stackoverflow.com/questions/4219255/how-do-you-get-the-list-of-targets-in-a-makefile
# no .PHONY because this will only be run before `make help`
# it's a separate command because getting the `info` takes a bit of time
# and we want to show the make targets right away without making the user wait for `info` to finish running
help-targets:
	@echo -n "# make targets"
	@echo ":"
	@cat makefile | sed -n '/^\.PHONY: / h; /\(^\t@*echo\|^\t:\)/ {H; x; /PHONY/ s/.PHONY: \(.*\)\n.*"\(.*\)"/    make \1\t\2/p; d; x}'| sort -k2,2 |expand -t 30


.PHONY: info
info: gen-version-info
	@echo "# makefile variables"
	@echo "    PYTHON = $(PYTHON)"
	@echo "    PYTHON_VERSION = $(PYTHON_VERSION)"
	@echo "    PACKAGE_NAME = $(PACKAGE_NAME)"
	@echo "    PROJ_VERSION = $(PROJ_VERSION)"
	@echo "    LAST_VERSION = $(LAST_VERSION)"
	@echo "    PYTEST_OPTIONS = $(PYTEST_OPTIONS)"

.PHONY: info-long
info-long: info
	@echo "# other variables"
	@echo "    PUBLISH_BRANCH = $(PUBLISH_BRANCH)"
	@echo "    DOCS_DIR = $(DOCS_DIR)"
	@echo "    COVERAGE_REPORTS_DIR = $(COVERAGE_REPORTS_DIR)"
	@echo "    TESTS_DIR = $(TESTS_DIR)"
	@echo "    TESTS_TEMP_DIR = $(TESTS_TEMP_DIR)"
	@echo "    PYPROJECT = $(PYPROJECT)"
	@echo "    REQUIREMENTS_DIR = $(REQUIREMENTS_DIR)"
	@echo "    LOCAL_DIR = $(LOCAL_DIR)"
	@echo "    PYPI_TOKEN_FILE = $(PYPI_TOKEN_FILE)"
	@echo "    LAST_VERSION_FILE = $(LAST_VERSION_FILE)"
	@echo "    PYTHON_BASE = $(PYTHON_BASE)"
	@echo "    COMMIT_LOG_FILE = $(COMMIT_LOG_FILE)"
	@echo "    RUN_GLOBAL = $(RUN_GLOBAL)"
	@echo "    TYPECHECK_ARGS = $(TYPECHECK_ARGS)"
	@echo "    TYPECHECK_PATH = $(TYPECHECK_PATH)"
	@echo "    TYPE_CHECKERS = $(TYPE_CHECKERS)"

# Smart help command: shows general help, or detailed info about specific targets
# Usage:
#   make help              - shows general help (list of targets + makefile variables)
#   make help="test"       - shows detailed info about the 'test' recipe
#   make HELP="test clean" - shows detailed info about multiple recipes
#   make h=*               - shows detailed info about all recipes (wildcard expansion)
#   make H="test"          - same as HELP (case variations supported)
#
# All variations work: help/HELP/h/H with values like "foo", "foo bar", "*", "--all"
.PHONY: help
help:
	@$(eval HELP_ARG := $(or $(HELP),$(help),$(H),$(h)))
	@$(eval HELP_EXPANDED := $(if $(filter *,$(HELP_ARG)),--all,$(HELP_ARG)))
	@if [ -n "$(HELP_EXPANDED)" ]; then \
		$(PYTHON_BASE) $(SCRIPTS_DIR)/recipe_info.py -f makefile $(HELP_EXPANDED); \
	else \
		$(MAKE) --no-print-directory help-targets info; \
		echo ""; \
		echo "To get detailed info about specific make targets or variables, use:"; \
		echo "  make help=TARGET    or    make HELP=\"TARGET1 TARGET2\""; \
		echo "  make help=VARIABLE  - shows variable values (case-insensitive)"; \
		echo "  make H=*            or    make h=--all"; \
	fi


 ######  ##     ##  ######  ########  #######  ##     ##
##    ## ##     ## ##    ##    ##    ##     ## ###   ###
##       ##     ## ##          ##    ##     ## #### ####
##       ##     ##  ######     ##    ##     ## ## ### ##
##       ##     ##       ##    ##    ##     ## ##     ##
##    ## ##     ## ##    ##    ##    ##     ## ##     ##
 ######   #######   ######     ##     #######  ##     ##

# ==================================================
# custom targets
# ==================================================
# (put them down here, or delimit with ~~~~~)
``````{ end_of_file="makefile" }

``````{ path="makefile.template"  }
#|==================================================================|
#| python project makefile template                                 |
#| originally by Michael Ivanitskiy (mivanits@umich.edu)            |
#| https://github.com/mivanit/python-project-makefile-template      |
##[[VERSION]]##
#| license: https://creativecommons.org/licenses/by-sa/4.0/         |
#|==================================================================|
#| CUSTOMIZATION:                                                   |
#| - modify PACKAGE_NAME and other variables in config section      |
#| - mark custom changes with `~~~~~` for easier template updates   |
#| - run `make help` to see available targets                       |
#| - run `make help=TARGET` for detailed info about specific target |
#|==================================================================|


 ######  ########  ######
##    ## ##       ##    ##
##       ##       ##
##       ######   ##   ####
##       ##       ##    ##
##    ## ##       ##    ##
 ######  ##        ######

# ==================================================
# configuration & variables
# ==================================================


# !!! MODIFY AT LEAST THIS PART TO SUIT YOUR PROJECT !!!
# it assumes that the source is in a directory named the same as the package name
# this also gets passed to some other places
PACKAGE_NAME := myproject

# for checking you are on the right branch when publishing
PUBLISH_BRANCH := main

# where to put docs
# if you change this, you must also change pyproject.toml:tool.makefile.docs.output_dir to match
DOCS_DIR := docs

# where the tests are, for pytest
TESTS_DIR := tests

# tests temp directory to clean up. will remove this in `make clean`
TESTS_TEMP_DIR := $(TESTS_DIR)/_temp/

# probably don't change these:
# --------------------------------------------------

# where the pyproject.toml file is. no idea why you would change this but just in case
PYPROJECT := pyproject.toml

# dir to store various configuration files
# use of `.meta/` inspired by https://news.ycombinator.com/item?id=36472613
META_DIR := .meta

# scripts download configuration
# override SCRIPTS_VERSION to download a specific version (e.g., make self-setup-scripts SCRIPTS_VERSION=v0.5.0)
SCRIPTS_DIR := $(META_DIR)/scripts
SCRIPTS_VERSION ?= main
SCRIPTS_REPO := mivanit/python-project-makefile-template
SCRIPTS_URL_BASE := https://raw.githubusercontent.com/$(SCRIPTS_REPO)/$(SCRIPTS_VERSION)/scripts/out

# requirements.txt files for base package, all extras, dev, and all
REQUIREMENTS_DIR := $(META_DIR)/requirements

# local files (don't push this to git!)
LOCAL_DIR := $(META_DIR)/local

# will print this token when publishing. make sure not to commit this file!!!
PYPI_TOKEN_FILE := $(LOCAL_DIR)/.pypi-token

# version files
VERSIONS_DIR := $(META_DIR)/versions

# the last version that was auto-uploaded. will use this to create a commit log for version tag
# see `gen-commit-log` target
LAST_VERSION_FILE := $(VERSIONS_DIR)/.lastversion

# current version (writing to file needed due to shell escaping issues)
VERSION_FILE := $(VERSIONS_DIR)/.version

# base python to use. Will add `uv run` in front of this if `RUN_GLOBAL` is not set to 1
PYTHON_BASE := python

# where the commit log will be stored
COMMIT_LOG_FILE := $(LOCAL_DIR)/.commit_log

# where to put the coverage reports
# note that this will be published with the docs!
# modify the `docs` targets and `.gitignore` if you don't want that
COVERAGE_REPORTS_DIR := $(DOCS_DIR)/coverage

# this stuff in the docs will be kept
# in addition to anything specified in `pyproject.toml:tool.makefile.docs.no_clean`
DOCS_RESOURCES_DIR := $(DOCS_DIR)/resources

# location of the make docs script
MAKE_DOCS_SCRIPT_PATH := $(SCRIPTS_DIR)/make_docs.py

# version vars - extracted automatically from `pyproject.toml`, `$(LAST_VERSION_FILE)`, and $(PYTHON)
# --------------------------------------------------

# assuming your `pyproject.toml` has a line that looks like `version = "0.0.1"`, `gen-version-info` will extract this
PROJ_VERSION := NULL
# `gen-version-info` will read the last version from `$(LAST_VERSION_FILE)`, or `NULL` if it doesn't exist
LAST_VERSION := NULL
# get the python version, now that we have picked the python command
PYTHON_VERSION := NULL


# ==================================================
# reading command line options
# ==================================================


# for formatting or something, we might want to run python without uv
# RUN_GLOBAL=1 to use global `PYTHON_BASE` instead of `uv run $(PYTHON_BASE)`
RUN_GLOBAL ?= 0

# for running tests or other commands without updating the env, set this to 1
# and it will pass `--no-sync` to `uv run`
UV_NOSYNC ?= 0

ifeq ($(RUN_GLOBAL),0)
	ifeq ($(UV_NOSYNC),1)
		PYTHON = uv run --no-sync $(PYTHON_BASE)
	else
		PYTHON = uv run $(PYTHON_BASE)
	endif
else
	PYTHON = $(PYTHON_BASE)
endif

# if you want different behavior for different python versions
# --------------------------------------------------
# COMPATIBILITY_MODE := $(shell $(PYTHON) -c "import sys; print(1 if sys.version_info < (3, 10) else 0)")

# options we might want to pass to pytest
# --------------------------------------------------

# base options for pytest, user can set this when running make to add more options
PYTEST_OPTIONS ?=

# type checker configuration
# --------------------------------------------------

# which type checkers to run (comma-separated)
# available: mypy,basedpyright,ty
TYPE_CHECKERS ?= mypy,basedpyright,ty

# path to type check (empty = use config from pyproject.toml)
TYPECHECK_PATH ?=

# directory to store type checker outputs
TYPE_ERRORS_DIR := $(META_DIR)/.type-errors

# typing summary output file
TYPING_SUMMARY_FILE := $(META_DIR)/typing-summary.toml

# ==================================================
# default target (help)
# ==================================================


# first/default target is help
.PHONY: default
default: help



 ######   ######  ########  #### ########  ########  ######
##    ## ##    ## ##     ##  ##  ##     ##    ##    ##    ##
##       ##       ##     ##  ##  ##     ##    ##    ##
 ######  ##       ########   ##  ########     ##     ######
      ## ##       ##   ##    ##  ##           ##          ##
##    ## ##    ## ##    ##   ##  ##           ##    ##    ##
 ######   ######  ##     ## #### ##           ##     ######

# ==================================================
# downloading scripts from github
# ==================================================

# list of scripts to download when running `make self-setup-scripts`. these are the helper scripts that the makefile uses for various tasks (e.g., getting version info, generating docs, etc.)
SCRIPTS_LIST := export_requirements get_version get_commit_log check_torch get_todos pdoc_markdown2_cli docs_clean typing_breakdown recipe_info make_docs generate_badge

# download makefile helper scripts from GitHub
# uses curl to fetch scripts from the template repository
# override version: make self-setup-scripts SCRIPTS_VERSION=v0.5.0
.PHONY: self-setup-scripts
self-setup-scripts:
	@echo "downloading makefile scripts (version: $(SCRIPTS_VERSION))"
	@mkdir -p $(SCRIPTS_DIR)
	@for script in $(SCRIPTS_LIST); do \
		echo "  $$script.py"; \
		curl -fsSL "$(SCRIPTS_URL_BASE)/$$script.py" -o "$(SCRIPTS_DIR)/$$script.py"; \
	done
	@echo "$(SCRIPTS_VERSION)" > $(SCRIPTS_DIR)/VERSION
	@echo "done"

##     ## ######## ########   ######  ####  #######  ##    ##
##     ## ##       ##     ## ##    ##  ##  ##     ## ###   ##
##     ## ##       ##     ## ##        ##  ##     ## ####  ##
##     ## ######   ########   ######   ##  ##     ## ## ## ##
 ##   ##  ##       ##   ##         ##  ##  ##     ## ##  ####
  ## ##   ##       ##    ##  ##    ##  ##  ##     ## ##   ###
   ###    ######## ##     ##  ######  ####  #######  ##    ##

# ==================================================
# getting version info
# we do this in a separate target because it takes a bit of time
# ==================================================


# this recipe is weird. we need it because:
# - a one liner for getting the version with toml is unwieldy, and using regex is fragile
# - using $$SCRIPT_GET_VERSION within $(shell ...) doesn't work because of escaping issues
# - trying to write to the file inside the `gen-version-info` recipe doesn't work, 
#   shell eval happens before our `python ...` gets run and `cat` doesn't see the new file
.PHONY: write-proj-version
write-proj-version:
	@mkdir -p $(VERSIONS_DIR)
	@$(PYTHON) $(SCRIPTS_DIR)/get_version.py "$(PYPROJECT)" > $(VERSION_FILE)

# gets version info from $(PYPROJECT), last version from $(LAST_VERSION_FILE), and python version
# uses just `python` for everything except getting the python version. no echo here, because this is "private"
.PHONY: gen-version-info
gen-version-info: write-proj-version
	@mkdir -p $(LOCAL_DIR)
	$(eval PROJ_VERSION := $(shell cat $(VERSION_FILE)) )
	$(eval LAST_VERSION := $(shell [ -f $(LAST_VERSION_FILE) ] && cat $(LAST_VERSION_FILE) || echo NULL) )
	$(eval PYTHON_VERSION := $(shell $(PYTHON) -c "import sys; print(f'{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}')") )

# getting commit log since the tag specified in $(LAST_VERSION_FILE)
# will write to $(COMMIT_LOG_FILE)
# when publishing, the contents of $(COMMIT_LOG_FILE) will be used as the tag description (but can be edited during the process)
# no echo here, because this is "private"
.PHONY: gen-commit-log
gen-commit-log: gen-version-info
	@if [ "$(LAST_VERSION)" = "NULL" ]; then \
		echo "!!! ERROR !!!"; \
		echo "LAST_VERSION is NULL, cant get commit log!"; \
		exit 1; \
	fi
	@mkdir -p $(LOCAL_DIR)
	@$(PYTHON) $(SCRIPTS_DIR)/get_commit_log.py "$(LAST_VERSION)" "$(COMMIT_LOG_FILE)"


# force the version info to be read, printing it out
# also force the commit log to be generated, and cat it out
.PHONY: version
version: gen-commit-log
	@echo "Current version is $(PROJ_VERSION), last auto-uploaded version is $(LAST_VERSION)"
	@echo "Commit log since last version from '$(COMMIT_LOG_FILE)':"
	@cat $(COMMIT_LOG_FILE)
	@echo ""
	@if [ "$(PROJ_VERSION)" = "$(LAST_VERSION)" ]; then \
		echo "!!! ERROR !!!"; \
		echo "Python package $(PROJ_VERSION) is the same as last published version $(LAST_VERSION), exiting!"; \
		exit 1; \
	fi



########  ######## ########   ######
##     ## ##       ##     ## ##    ##
##     ## ##       ##     ## ##
##     ## ######   ########   ######
##     ## ##       ##              ##
##     ## ##       ##        ##    ##
########  ######## ##         ######

# ==================================================
# dependencies and setup
# ==================================================


.PHONY: setup
setup: self-setup-scripts dep-check
	@echo "download scripts and sync dependencies"
	@echo ""
	@echo "setup complete! To activate the virtual environment, run one of:"
	@echo "  source .venv/bin/activate"
	@echo "  source .venv/Scripts/activate"

.PHONY: dep-check-torch
dep-check-torch:
	@echo "see if torch is installed, and which CUDA version and devices it sees"
	$(PYTHON) $(SCRIPTS_DIR)/check_torch.py

# sync dependencies and export to requirements.txt files
# - syncs all extras and groups with uv (including dev dependencies)
# - compiles bytecode for faster imports
# - exports to requirements.txt files per tool.uv-exports.exports config
# configure via pyproject.toml:[tool.uv-exports]:
#   [tool.uv-exports]
#   exports = [
#     { name = "base", extras = [], groups = [] },  # base package deps only
#     { name = "dev", extras = [], groups = ["dev"] },  # dev dependencies
#     { name = "all", extras = ["all"], groups = ["dev"] }  # everything
#   ]
.PHONY: dep
dep:
	@echo "Exporting dependencies as per $(PYPROJECT) section 'tool.uv-exports.exports'"
	uv sync --all-extras --all-groups --compile-bytecode
	mkdir -p $(REQUIREMENTS_DIR)
	$(PYTHON) $(SCRIPTS_DIR)/export_requirements.py $(PYPROJECT) $(REQUIREMENTS_DIR) | sh -x


# verify that requirements.txt files match current dependencies
# - exports deps to temp directory
# - diffs temp against existing requirements files
# - FAILS if any differences found (means you need to run `make dep`)
# useful in CI to catch when pyproject.toml changed but requirements weren't regenerated
.PHONY: dep-check
dep-check:
	@echo "Checking that exported requirements are up to date"
	uv sync --all-extras --all-groups
	mkdir -p $(REQUIREMENTS_DIR)-TEMP
	$(PYTHON) $(SCRIPTS_DIR)/export_requirements.py $(PYPROJECT) $(REQUIREMENTS_DIR)-TEMP | sh -x
	diff -r $(REQUIREMENTS_DIR)-TEMP $(REQUIREMENTS_DIR)
	rm -rf $(REQUIREMENTS_DIR)-TEMP


.PHONY: dep-clean
dep-clean:
	@echo "clean up lock files, .venv, and requirements files"
	rm -rf .venv
	rm -rf uv.lock
	rm -rf $(REQUIREMENTS_DIR)/*.txt


 ######  ##     ## ########  ######  ##    ##  ######
##    ## ##     ## ##       ##    ## ##   ##  ##    ##
##       ##     ## ##       ##       ##  ##   ##
##       ######### ######   ##       #####     ######
##       ##     ## ##       ##       ##  ##         ##
##    ## ##     ## ##       ##    ## ##   ##  ##    ##
 ######  ##     ## ########  ######  ##    ##  ######

# ==================================================
# checks (formatting/linting, typing, tests)
# ==================================================


# format code AND auto-fix linting issues
# performs TWO operations: reformats code, then auto-fixes safe linting issues
# configure in pyproject.toml:[tool.ruff]
.PHONY: format
format:
	@echo "format the source code"
	$(PYTHON) -m ruff format --config $(PYPROJECT) .
	$(PYTHON) -m ruff check --fix --config $(PYPROJECT) .

# runs ruff to check if the code is formatted correctly
.PHONY: format-check
format-check:
	@echo "check if the source code is formatted correctly"
	$(PYTHON) -m ruff check --config $(PYPROJECT) .

# runs type checks with configured checkers
# set TYPE_CHECKERS to customize which checkers run (e.g., TYPE_CHECKERS=mypy,basedpyright)
# set TYPING_OUTPUT_DIR to save outputs to files (used by typing-summary)
# returns exit code 1 if any checker fails
.PHONY: typing
typing:
	@echo "running type checks"
	@div="--------------------------------------------------"; \
	failed=0; \
	for c in $$(echo "$(TYPE_CHECKERS)" | tr ',' ' '); do \
		case "$$c" in ty) subcmd="check";; *) subcmd="";; esac; \
		printf "\033[36m$$div\n[$$c]\n$$div\033[0m\n"; \
		$(PYTHON) -m $$c $$subcmd $(TYPECHECK_ARGS) $(TYPECHECK_PATH) $(if $(TYPING_OUTPUT_DIR),> $(TYPING_OUTPUT_DIR)/$$c.txt 2>&1) || failed=1; \
	done; \
	if [ $$failed -eq 1 ]; then \
		printf "\033[31m$$div\nnot all type checks passed\n$$div\033[0m\n"; \
		exit 1; \
	else \
		printf "\033[32m$$div\nall type checks passed\n$$div\033[0m\n"; \
	fi

# save type check outputs and generate detailed breakdown
# outputs are saved to $(TYPE_ERRORS_DIR)/*.txt
# summary is generated to $(TYPING_SUMMARY_FILE)
.PHONY: typing-summary
typing-summary:
	@echo "running type checks and saving to $(TYPE_ERRORS_DIR)/"
	@mkdir -p $(TYPE_ERRORS_DIR)
	-@$(MAKE) --no-print-directory typing TYPING_OUTPUT_DIR=$(TYPE_ERRORS_DIR)
	@echo "generating typing summary..."
	$(PYTHON) $(SCRIPTS_DIR)/typing_breakdown.py --error-dir $(TYPE_ERRORS_DIR) --output $(TYPING_SUMMARY_FILE) --checkers $(TYPE_CHECKERS)

# run tests with pytest
# you can pass custom args. for example:
# make test PYTEST_OPTIONS="--maxfail=1 -x"
# pytest config in pyproject.toml:[tool.pytest.ini_options]
.PHONY: test
test:
	@echo "running tests"
	$(PYTHON) -m pytest $(PYTEST_OPTIONS) $(TESTS_DIR)

.PHONY: check
check: format-check test typing
	@echo "run format checks, tests, and typing checks"


########   #######   ######   ######
##     ## ##     ## ##    ## ##    ##
##     ## ##     ## ##       ##
##     ## ##     ## ##        ######
##     ## ##     ## ##             ##
##     ## ##     ## ##    ## ##    ##
########   #######   ######   ######

# ==================================================
# coverage & docs
# ==================================================


# generates a whole tree of documentation in html format.
# see `$(MAKE_DOCS_SCRIPT_PATH)` and the templates in `$(DOCS_RESOURCES_DIR)/templates/html/` for more info
.PHONY: docs-html
docs-html:
	@echo "generate html docs"
	$(PYTHON) $(MAKE_DOCS_SCRIPT_PATH)

# instead of a whole website, generates a single markdown file with all docs using the templates in `$(DOCS_RESOURCES_DIR)/templates/markdown/`.
# this is useful if you want to have a copy that you can grep/search, but those docs are much messier.
.PHONY: docs-md
docs-md:
	@echo "generate combined (single-file) docs in markdown"
	mkdir $(DOCS_DIR)/combined -p
	$(PYTHON) $(MAKE_DOCS_SCRIPT_PATH) --combined

# generate coverage reports from test results
# WARNING: if .coverage file not found, will automatically run `make test` first
# - generates text report: $(COVERAGE_REPORTS_DIR)/coverage.txt
# - generates SVG badge: $(COVERAGE_REPORTS_DIR)/coverage.svg
# - generates HTML report: $(COVERAGE_REPORTS_DIR)/html/
# - removes .gitignore from html dir (we publish coverage with docs)
.PHONY: cov
cov:
	@echo "generate coverage reports"
	@if [ ! -f .coverage ]; then \
		echo ".coverage not found, running tests first..."; \
		$(MAKE) test PYTEST_OPTIONS="$(PYTEST_OPTIONS) --cov=." ; \
	fi
	mkdir $(COVERAGE_REPORTS_DIR) -p
	$(PYTHON) -m coverage report -m > $(COVERAGE_REPORTS_DIR)/coverage.txt
	$(PYTHON) $(SCRIPTS_DIR)/generate_badge.py --coverage $(COVERAGE_REPORTS_DIR)/coverage.txt > $(COVERAGE_REPORTS_DIR)/coverage.svg
	$(PYTHON) -m coverage html --directory=$(COVERAGE_REPORTS_DIR)/html/
	rm -rf $(COVERAGE_REPORTS_DIR)/html/.gitignore

# runs the coverage report, then the docs, then the combined docs
.PHONY: docs
docs: cov docs-html docs-md todo lmcat
	@echo "generate all documentation and coverage reports"

# remove generated documentation files, but preserve resources
# - removes all docs except those in DOCS_RESOURCES_DIR
# - preserves files/patterns specified in pyproject.toml config
# - distinct from `make clean` (which removes temp build files, not docs)
# configure via pyproject.toml:[tool.makefile.docs]:
#   [tool.makefile.docs]
#   output_dir = "docs"  # must match DOCS_DIR in makefile
#   no_clean = [  # files/patterns to preserve when cleaning
#     "resources/**",
#     "*.svg",
#     "*.css"
#   ]
.PHONY: docs-clean
docs-clean:
	@echo "remove generated docs except resources"
	$(PYTHON) $(SCRIPTS_DIR)/docs_clean.py $(PYPROJECT) $(DOCS_DIR) $(DOCS_RESOURCES_DIR)


# get all TODO's from the code
# configure via pyproject.toml:[tool.makefile.inline-todo]:
#   [tool.makefile.inline-todo]
#   search_dir = "."  # directory to search for TODOs
#   out_file_base = "docs/other/todo-inline"  # output file path (without extension)
#   context_lines = 2  # lines of context around each TODO
#   extensions = ["py", "md"]  # file extensions to search
#   tags = ["CRIT", "TODO", "FIXME", "HACK", "BUG", "DOC"]  # tags to look for
#   exclude = ["docs/**", ".venv/**", "scripts/get_todos.py"]  # patterns to exclude
#   branch = "main"  # git branch for URLs
#   # repo_url = "..."  # repository URL (defaults to [project.urls.{repository,github}])
#   # template_md = "..."  # custom jinja2 template for markdown output
#   # template_issue = "..."  # custom format string for issues
#   # template_html_source = "..."  # custom html template path
#   tag_label_map = { "BUG" = "bug", "TODO" = "enhancement", "DOC" = "documentation" } # mapping of tags to GitHub issue labels
.PHONY: todo
todo:
	@echo "get all TODO's from the code"
	$(PYTHON) $(SCRIPTS_DIR)/get_todos.py

.PHONY: lmcat-tree
lmcat-tree:
	@echo "show in console the lmcat tree view"
	-$(PYTHON) -m lmcat -t --output STDOUT

.PHONY: lmcat
lmcat:
	@echo "write the lmcat full output to pyproject.toml:[tool.lmcat.output]"
	-$(PYTHON) -m lmcat

########  ##     ## #### ##       ########
##     ## ##     ##  ##  ##       ##     ##
##     ## ##     ##  ##  ##       ##     ##
########  ##     ##  ##  ##       ##     ##
##     ## ##     ##  ##  ##       ##     ##
##     ## ##     ##  ##  ##       ##     ##
########   #######  #### ######## ########

# ==================================================
# build and publish
# ==================================================


# verify git is ready for publishing
# REQUIRES:
# - current branch must be $(PUBLISH_BRANCH)
# - no uncommitted changes (git status --porcelain must be empty)
# EXITS with error if either condition fails
.PHONY: verify-git
verify-git:
	@echo "checking git status"
	if [ "$(shell git branch --show-current)" != $(PUBLISH_BRANCH) ]; then \
		echo "!!! ERROR !!!"; \
		echo "Git is not on the $(PUBLISH_BRANCH) branch, exiting!"; \
		git branch; \
		git status; \
		exit 1; \
	fi; \
	if [ -n "$(shell git status --porcelain)" ]; then \
		echo "!!! ERROR !!!"; \
		echo "Git is not clean, exiting!"; \
		git status; \
		exit 1; \
	fi; \


# build package distribution files
# creates wheel (.whl) and source distribution (.tar.gz) in dist/
.PHONY: build
build:
	@echo "build the package"
	uv build

# publish package to PyPI and create git tag
# PREREQUISITES:
# - must be on $(PUBLISH_BRANCH) branch with clean git status (verified by verify-git)
# - must have $(PYPI_TOKEN_FILE) with your PyPI token
# - version in pyproject.toml must be different from $(LAST_VERSION_FILE)
# PROCESS:
# 1. runs checks, validates version, builds package, verifies git clean
# 2. prompts for version confirmation (you can edit $(COMMIT_LOG_FILE) at this point)
# 3. creates git commit updating $(LAST_VERSION_FILE)
# 4. creates annotated git tag with commit log as description
# 5. pushes tag to origin
# 6. uploads to PyPI via twine
.PHONY: publish
publish: check version build verify-git
	@echo "Ready to publish $(PROJ_VERSION) to PyPI"
	@echo "Now would be the time to edit $(COMMIT_LOG_FILE) for the tag description"

	@read -p "Enter version to confirm: " NEW_VERSION && \
	if [ "$$NEW_VERSION" != "$(PROJ_VERSION)" ]; then \
		echo "Version mismatch: got $$NEW_VERSION, expected $(PROJ_VERSION)"; \
		exit 1; \
	fi && \
	echo "Version confirmed."

	@test -f "$(PYPI_TOKEN_FILE)" || { echo "ERROR: Token file not found at $(PYPI_TOKEN_FILE)"; exit 1; }

	@echo "Committing and tagging..."
	echo $(PROJ_VERSION) > $(LAST_VERSION_FILE) && \
	git add $(LAST_VERSION_FILE) && \
	git commit -m "Auto update to $(PROJ_VERSION)" && \
	git tag -a $(PROJ_VERSION) -F $(COMMIT_LOG_FILE) && \
	git push origin $(PROJ_VERSION)

	@echo "Uploading to PyPI..."
	TWINE_USERNAME=__token__ TWINE_PASSWORD="$$(cat $(PYPI_TOKEN_FILE))" $(PYTHON) -m twine upload dist/* --verbose

	@echo "Published $(PROJ_VERSION) successfully!"

# ==================================================
# cleanup of temp files
# ==================================================


# cleans up temporary files:
# - caches: .mypy_cache, .ruff_cache, .pytest_cache, .coverage
# - build artifacts: dist/, build/, *.egg-info
# - test temp files: $(TESTS_TEMP_DIR)
# - __pycache__ directories and *.pyc/*.pyo files in $(PACKAGE_NAME), $(TESTS_DIR), $(DOCS_DIR)
# uses `-` prefix on find commands to continue even if directories don't exist
# distinct from `make docs-clean`, which removes generated documentation
.PHONY: clean
clean:
	@echo "clean up temporary files"
	rm -rf .mypy_cache .ruff_cache .pytest_cache .coverage dist build $(PACKAGE_NAME).egg-info $(TESTS_TEMP_DIR) $(TYPE_ERRORS_DIR)
	-find $(PACKAGE_NAME) $(TESTS_DIR) $(DOCS_DIR) -type d -name '__pycache__' -exec rm -rf {} +
	-find $(PACKAGE_NAME) $(TESTS_DIR) $(DOCS_DIR) -type f -name '*.py[co]' -delete

# remove all generated/build files including .venv
# runs: clean + docs-clean + dep-clean
# removes .venv, uv.lock, requirements.txt files, generated docs, build artifacts
# run `make dep` after this to reinstall dependencies
.PHONY: clean-all
clean-all: clean docs-clean dep-clean
	@echo "clean up all temporary files, dep files, venv, and generated docs"


##     ## ######## ##       ########
##     ## ##       ##       ##     ##
##     ## ##       ##       ##     ##
######### ######   ##       ########
##     ## ##       ##       ##
##     ## ##       ##       ##
##     ## ######## ######## ##

# ==================================================
# smart help command
# ==================================================


# listing targets is from stackoverflow
# https://stackoverflow.com/questions/4219255/how-do-you-get-the-list-of-targets-in-a-makefile
# no .PHONY because this will only be run before `make help`
# it's a separate command because getting the `info` takes a bit of time
# and we want to show the make targets right away without making the user wait for `info` to finish running
help-targets:
	@echo -n "# make targets"
	@echo ":"
	@cat makefile | sed -n '/^\.PHONY: / h; /\(^\t@*echo\|^\t:\)/ {H; x; /PHONY/ s/.PHONY: \(.*\)\n.*"\(.*\)"/    make \1\t\2/p; d; x}'| sort -k2,2 |expand -t 30


.PHONY: info
info: gen-version-info
	@echo "# makefile variables"
	@echo "    PYTHON = $(PYTHON)"
	@echo "    PYTHON_VERSION = $(PYTHON_VERSION)"
	@echo "    PACKAGE_NAME = $(PACKAGE_NAME)"
	@echo "    PROJ_VERSION = $(PROJ_VERSION)"
	@echo "    LAST_VERSION = $(LAST_VERSION)"
	@echo "    PYTEST_OPTIONS = $(PYTEST_OPTIONS)"

.PHONY: info-long
info-long: info
	@echo "# other variables"
	@echo "    PUBLISH_BRANCH = $(PUBLISH_BRANCH)"
	@echo "    DOCS_DIR = $(DOCS_DIR)"
	@echo "    COVERAGE_REPORTS_DIR = $(COVERAGE_REPORTS_DIR)"
	@echo "    TESTS_DIR = $(TESTS_DIR)"
	@echo "    TESTS_TEMP_DIR = $(TESTS_TEMP_DIR)"
	@echo "    PYPROJECT = $(PYPROJECT)"
	@echo "    REQUIREMENTS_DIR = $(REQUIREMENTS_DIR)"
	@echo "    LOCAL_DIR = $(LOCAL_DIR)"
	@echo "    PYPI_TOKEN_FILE = $(PYPI_TOKEN_FILE)"
	@echo "    LAST_VERSION_FILE = $(LAST_VERSION_FILE)"
	@echo "    PYTHON_BASE = $(PYTHON_BASE)"
	@echo "    COMMIT_LOG_FILE = $(COMMIT_LOG_FILE)"
	@echo "    RUN_GLOBAL = $(RUN_GLOBAL)"
	@echo "    TYPECHECK_ARGS = $(TYPECHECK_ARGS)"
	@echo "    TYPECHECK_PATH = $(TYPECHECK_PATH)"
	@echo "    TYPE_CHECKERS = $(TYPE_CHECKERS)"

# Smart help command: shows general help, or detailed info about specific targets
# Usage:
#   make help              - shows general help (list of targets + makefile variables)
#   make help="test"       - shows detailed info about the 'test' recipe
#   make HELP="test clean" - shows detailed info about multiple recipes
#   make h=*               - shows detailed info about all recipes (wildcard expansion)
#   make H="test"          - same as HELP (case variations supported)
#
# All variations work: help/HELP/h/H with values like "foo", "foo bar", "*", "--all"
.PHONY: help
help:
	@$(eval HELP_ARG := $(or $(HELP),$(help),$(H),$(h)))
	@$(eval HELP_EXPANDED := $(if $(filter *,$(HELP_ARG)),--all,$(HELP_ARG)))
	@if [ -n "$(HELP_EXPANDED)" ]; then \
		$(PYTHON_BASE) $(SCRIPTS_DIR)/recipe_info.py -f makefile $(HELP_EXPANDED); \
	else \
		$(MAKE) --no-print-directory help-targets info; \
		echo ""; \
		echo "To get detailed info about specific make targets or variables, use:"; \
		echo "  make help=TARGET    or    make HELP=\"TARGET1 TARGET2\""; \
		echo "  make help=VARIABLE  - shows variable values (case-insensitive)"; \
		echo "  make H=*            or    make h=--all"; \
	fi


 ######  ##     ##  ######  ########  #######  ##     ##
##    ## ##     ## ##    ##    ##    ##     ## ###   ###
##       ##     ## ##          ##    ##     ## #### ####
##       ##     ##  ######     ##    ##     ## ## ### ##
##       ##     ##       ##    ##    ##     ## ##     ##
##    ## ##     ## ##    ##    ##    ##     ## ##     ##
 ######   #######   ######     ##     #######  ##     ##

# ==================================================
# custom targets
# ==================================================
# (put them down here, or delimit with ~~~~~)
``````{ end_of_file="makefile.template" }

``````{ path="pyproject.toml"  }
[project]
	# basic information, URLs down below because toml doesn't let you define the same table twice
	name = "myproject"
	version = "0.5.0"
	description = "template for python projects/packages"
	authors = [
		{ name = "Michael Ivanitskiy", email = "mivanits@umich.edu" }
	]
	readme = "README.md"
	requires-python = ">=3.9"

	# dependencies

	dependencies = [
		"numpy>=1.21.0",
		"torch>=1.13.0",
		"muutils>=0.7.0",
		"ipykernel>=6.29.5",
	]

[project.optional-dependencies]
	# handling torch
	# torch-cu124 = [ "torch>=1.13.0" ]

	# for example purposes only
	cli = [
		"fire>=0.6.0",
	]

[dependency-groups]
	dev = [
		# test
		"pytest>=8.2.2",
		# coverage
		"pytest-cov>=4.1.0",
		# type checking
		"mypy>=1.0.1",
		"ty",
		"basedpyright",
		# docs
		'pdoc>=14.6.0',
		"nbconvert>=7.16.4", # for notebooks
		# lmcat -- a custom library. not exactly docs, but lets an LLM see all the code
		"lmcat>=0.2.0; python_version >= '3.11'",
		# tomli since no tomlib in python < 3.11
		"tomli>=2.1.0; python_version < '3.11'",
		# twine
		"twine",
		# jinja2 vuln, see
		# https://github.com/mivanit/python-project-makefile-template/security/dependabot/4
		"jinja2>=3.1.6",
	]
	lint = [
		# lint
		"ruff>=0.4.8",
	]


# more project metadata

[project.urls]

	Homepage = "https://miv.name/python-project-makefile-template"
	Documentation = "https://miv.name/python-project-makefile-template"
	Repository = "https://github.com/mivanit/python-project-makefile-template"
	Issues = "https://github.com/mivanit/python-project-makefile-template/issues"


# uv
[tool.uv]
	default-groups = ["dev", "lint"]

	# [tool.uv.sources]
	# 	torch = [ { index = "pytorch-cu124", extra = "torch-cu124" } ]

	# [[tool.uv.index]]
	# name = "pytorch-cu124"
	# url = "https://download.pytorch.org/whl/cu124"
	# explicit = true

[build-system]
	requires = ["hatchling"]
	build-backend = "hatchling.build"


# mypy config
[tool.mypy]
	files = ["scripts/make"]
	exclude = [
		".*/__pycache__/.*",
		"docs/.*",
		"scripts/out/.*",
		".meta/scripts/.*",
		".venv/.*",
		".mypy_cache/.*",
		".ruff_cache/.*",
	]


# ruff config
[tool.ruff]
	exclude = [
		"__pycache__",
		".meta/scripts",
		"scripts/out",
	]

	[tool.ruff.format]
		indent-style = "tab"
		skip-magic-trailing-comma = false

	[tool.ruff.lint]
		ignore = [
			"COM812", # ????
			"F722", # doesn't like jaxtyping
			"W191", # we like tabs
			"D400", # missing-trailing-period
			"D415", # missing-terminal-punctuation
			"E501", # line-too-long
			"S101", # assert is fine
			"D403", # first-word-uncapitalized
			"D206", # docstring-tab-indentation
			"ERA001", # commented-out-code
			"T201", # print is fine lmao
			"C408", # calling dict() is fine
			"UP015", # we like specifying the mode even if it's the default
			"D300", # we like docstrings
			# boolean positional arguments are fine
			"FBT001", 
			"FBT002",
			"FBT003",
			"PTH123", # opening files is fine
			"RET505", # else return is fine
			"FIX002", # `make todo` will give us the TODO comments
			"PIE790", # be explicit about when we pass
			"EM101", # fine to have string literal exceptions
			"FURB129", # .readlines() is fine
			"SIM108", # ternary operators can be hard to read, choose on a case-by-case basis
			"PLR5501", # nested if else is fine, for readability
			"D203", # docstring right after the class
			"D213", # docstring on first line
			"NPY002", # legacy numpy generator is fine
			"D401", # dont care about imperative mood
			# todos:
			"TD001", # we allow tags besides "TODO"
			"TD002", # dont care about author
			"TD003", # `make todo` will give us a table where we can create issues
			"FIX001", # FIXME comments are ok since `make todo` handles them
			"PLR0913", # sometimes you have to have a lot of args
			"B028", # fine to omit stacklevel on warnings
			# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
			# only for old version compatibility
			"UP007", # `Optional` is ok, we might not want to use `|` for compatibility
			# old style hints `Tuple`, `List`, etc. are fine
			"UP006", 
			"UP035",
			# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
			# delete this section!
			"INP001", # this isn't a package, just some scripts
			"PGH003", # bare type ignore is fine
			"SLF001", # private member access is fine
			# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
		]
		select = ["ALL"]
		# select = ["ICN001"]

		[tool.ruff.lint.per-file-ignores]
			"scripts/make/*" = [
				"PLR2004",  # magic values (coverage thresholds are self-documenting)
				"RET504",   # unnecessary assignment before return
				"TRY003",   # long exception messages
				"TRY300",   # return in else block
				"BLE001",   # blind exception catch (appropriate for CLI main())
				"D103",     # missing docstring in main()
				"PTH109",   # os.getcwd() vs Path.cwd()
				"PTH100",   # os.path.abspath() vs Path.resolve()
			]
			"tests/*" = [
				# dont need docstrings in test functions or modules
				"D103", 
				"D100",
				"INP001", # dont need __init__ either
				# dont need type annotations in test functions
				"ANN001",
				"ANN201", 
				"ANN202",
				"TRY003", # long exception messages in tests are fine
			]
			"docs/*" = ["ALL"] # not our problem
			"**/*.ipynb" = [
				"D103", # dont need docstrings
				"PLR2004", # magic variables are fine
				"N806", # uppercase vars are fine
			]
			# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
			# delete this!
			# we use trailing whitespace intentionally in the markdown template
			"scripts/make/get_todos.py" = ["W291"]
			# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# basedpyright config
[tool.basedpyright]
	include = ["scripts/make"]
	exclude = [
		".meta/scripts",
		"scripts/out",
		".venv",
		"docs",
		".mypy_cache",
		".ruff_cache",
	]
	reportAny = "none"
	reportUnusedCallResult = "none"
	reportUnnecessaryIsInstance = "none"
	reportExplicitAny = "none"

# ty config
[tool.ty.src]
	include = ["scripts/make"]
	exclude = [
		".meta/scripts",
		"scripts/out",
		".venv",
		"docs",
		".mypy_cache",
		".ruff_cache",
	]
[tool.ty.rules]
	unused-type-ignore-comment = "ignore"

# `make lmcat` depends on the lmcat and can be configured here
[tool.lmcat]
	output = "docs/other/lmcat.txt" # changing this might mean it wont be accessible from the docs
	ignore_patterns = [
		"docs/**",
		".venv/**",
		".git/**",
		".meta/**",
		"uv.lock",
		"LICENSE",
		"scripts/out/",
	]

# for configuring this tool (makefile, make_docs.py)
# ============================================================
[tool.makefile]

# documentation configuration, for `make docs` and `make docs-clean`
[tool.makefile.docs]
    # Output directory for generated documentation
    # MUST match DOCS_DIR in makefile
    output_dir = "docs"

    # List of files/directories in docs/ that should not be cleaned by `make docs-clean`
    # These paths are relative to output_dir
    no_clean = [
        ".nojekyll",  # For GitHub Pages
        # "resources/", # Templates, CSS, etc. this, or whatever is specified as DOCS_RESOURCES_DIR in makefile will always be preserved
    ]

    # Increment level of markdown headings in generated documentation
    # e.g. if 2, then h1 -> h3, h2 -> h4, etc.
    markdown_headings_increment = 2

    # Warnings to ignore during documentation generation
    warnings_ignore = [
        ".*No docstring.*",
        ".*Private member.*",
    ]

    # optional generation of notebooks as html pages
    [tool.makefile.docs.notebooks]
        # Enable notebook processing in documentation
		# disabled by default
        enabled = true
        
        # Source directory containing .ipynb files
        source_path = "notebooks"
        
        # Output path relative to docs directory [tool.makefile.docs.output_dir]
        output_path_relative = "notebooks"
        
        # Custom template for notebooks index page
        # Available variables: notebook_url, notebooks (list of dicts with ipynb, html, desc)
        # index_template = ...

        # Descriptions for notebooks, shown in index
        [tool.makefile.docs.notebooks.descriptions]
            "example" = "Example notebook showing basic usage"
            "advanced" = "Advanced usage patterns and techniques"
        
        

# Custom export configurations
# affects `make dep` and related commands
[tool.makefile.uv-exports]
	args = [
		"--no-hashes"
	]
	exports = [
		# no groups, no extras, just the base dependencies
		{ name = "base", groups = false, extras = false },
		# all groups
		{ name = "groups", groups = true, extras = false },
		# only the lint group -- custom options for this
		{ name = "lint", options = ["--only-group", "lint"] },
		# # all groups and extras
		{ name = "all", filename="requirements.txt", groups = true, extras=true },
		# # all groups and extras, a different way
		{ name = "allalt", groups = true, options = ["--all-extras"] },
	]

# configures `make todo`
[tool.makefile.inline-todo]
	# Directory to search for TODOs
	search_dir = "."
	
	# Output file base path (relative to project root)
    # If changed, update docs references -- they point to 'docs/other/todo-inline.html'
	out_file_base = "docs/other/todo-inline"

	# Number of context lines to include around each TODO
	context_lines = 2

	# File extensions to search
	extensions = ["py", "md"]

	# Tags to look for
	tags = ["CRIT", "TODO", "FIXME", "HACK", "BUG", "DOC"]

	# Patterns to exclude from search
	exclude = [
		"docs/**",
		".venv/**",
		"scripts/get_todos.py",
	]
    
    # configuring the output
	# ------------------------------
	# branch to put in the url
	branch = "main"

	# repo url -- by default this will come from `[project.urls.{repository, github}]`
	# but you can override it here
	# repo_url = ...

    
	
	# template for the markdown output
	# this uses jinja2. see `TEMPLATE_MD` in makefile under `SCRIPT_GET_TODOS`
	# template_md = ...

	# this uses standard python string formatting
	# available variables: file, file_lang, line_num, code_url, context
	# template_issue = ...

	# this template has some custom syntax for adding the data directly to the html file. see that file for more info
	# template_html_source = "docs/resources/templates/todo-template.html"

    # Mapping of tags to GitHub issue labels
    [tool.makefile.inline-todo.tag_label_map]
        "BUG" = "bug"
        "TODO" = "enhancement"
		"DOC" = "documentation"

# ============================================================


``````{ end_of_file="pyproject.toml" }